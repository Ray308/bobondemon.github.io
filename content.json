[{"title":"WGAN Part 1: 先用 GAN 鋪梗","date":"2017-03-16T13:25:12.000Z","path":"2017/03/16/WGAN-Part-1/","text":"筆者才疏學淺，如有錯誤，還請指正 Generative Adversarial Nets 提出了一個 NN 的 generative modeling 方法，在這之前，NN 要成為 p.d.f. 必須依賴於 sigmoid activation 的 Restricted Boltzmann Machines (RBM) 結構。例如 Deep Belief Net，整個 network 才會是一個 p.d.f.。然而學習這樣的一個 p.d.f. 必須使用 Contrastive Divergence 的 MCMC 方法， model 訓練完後要產生 sample 時也還是必須依賴 MCMC。另外在實用上，偏偏 sigmoid 很多時候效果不如 ReLu, maxout 等，例如 sigmoid 有嚴重的 gradient vanish problem。這使得 NN 在 generative modeling 又或是 unsupervised learning 上一直困難重重。 GAN 一出立即打破這個難堪的限制 ! 怎麼說呢? GAN 捨棄能夠明確表達出 p.d.f.的作法，寫不出明確的 p.d.f. 一點也沒關係，只要能生成 夠真的sample點，並且sample的機率跟training data一樣就好 然而 GAN 在實作上卻會遇上一些困難，例如生成的 samples 多樣性不足，訓練流程/架構 和 hyper-parameters 需要小心選擇，無法明確知道訓練的收斂狀況，這些問題等下會說明。 本篇的主角 (事實上下一篇才會登場) Wasserstein GAN (WGAN)，從本質上探討 GAN 目標函式中使用的 distance measure，進而根本地解決上述三個問題，這大大降低了 generative modeling 訓練難度 ! 我們還是來談談 GAN 怎麼一回事先吧。 Generative Adversarial NetsGAN 使用一個 two-player minimax gaming 策略。先用直觀說，我們有一個 生成器 \\(G\\)，用來生成夠真的 sample，另外還有一個 鑑別器 \\(D\\)，用來分辨 sample 究竟是真實資料 (training data) 來的呢，還是假的 (\\(G\\)產生的)。當這兩個模型互相競爭到一個平衡點的時候，也就是 \\(G\\) 能夠產生到 \\(D\\) 分辨不出真假的 sample，我們的生成器 \\(G\\) 就鍊成了。而 GAN 作者厲害的地方就在於 一: 將這兩個model的競爭規則轉換成一個最佳化問題二: 並且證明，當達到賽局的平衡點時(達到最佳解)，生成器就鍊成 (可以完美表示 training data 的 pdf，並且可sampling) 我們還是必須把上述策略嚴謹的表達出來 (寫成最佳化問題)，並證明當達到最佳化問題的最佳解時，就剛好完成生成器的鍊成。 * Two-player Minimax Game原則上我們希望鑑別器 \\(D\\) 能分辨出真假 sample，因此 \\(D(x)\\) 很自然地可以表示為 sample \\(x\\) 為真的機率另外生成器 \\(G\\) 則是負責產生假 sample，也可以很自然地表達為 \\(G(z)\\)，其中 \\(z\\) 為 latent variables，且我們可以假設該 latent variables \\(z\\) follow 一個 prior distribution \\(p_z(z)\\)。 我們希望 \\(D(x)\\) 對來自於真實資料的 samples 能夠盡量大，而對來自於 \\(G\\) 產生的要盡量小，因此對於鑑別器來說，它的目標函式可定義為如下: $$\\begin{align} Maximize: E_{x \\sim p_{data}(x)} [\\log D(x)] + E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ 另一方面，我們希望 \\(G\\) 能夠強到讓 \\(D\\) 無法分辨真偽，因此生成器的目標函式為: $$\\begin{align} Minimize: E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ 結合上述兩個目標函式就是如下的 minmax problem了 $$\\begin{align} \\min_G{ \\max_D{V(D,G)} } = E_{x \\sim p_{data}(x)} [\\log D(x)] + E_{z \\sim p_z(z)}[\\log (1-D(G(z)))] \\end{align}$$ 這邊作者很漂亮地給出了上述問題的理論證明。證明了兩件事情: 上述最佳化問題 (式(3)) 達到 global optimum 時, \\( p_g = p_d \\)。 (生成器產生出來的 pdf 會等於真實資料的 pdf，因此生成器鍊成!) 使用如下的演算法可以找到 global optimum 接下來我們只討論第一個事情的證明，因為這關係到 GAN 的弱點，也就是 WGAN 要解決的問題根源! * 證明 Global optimum 發生時，鍊成生成器大方向是這樣的 A. 假如給定 \\(G\\)，我們都可以找到一個相對應的 \\(D_G^*\\) 最佳化鑑別器的目標函式 (1)。B. 改寫原來的目標函式 \\(V(G,D)\\)，改寫後只跟 \\(G\\) 有關，我們定義為 \\(C(G)\\)，這是因為對於每一個 \\(G\\) 我們已經配給它相對應的 \\(D_G^*\\) 了，接著證明最佳解只發生在 \\( p_g = p_d \\) 的情況。 步驟 A: $$V(G,D)=\\int_{x}{p_d(x)\\log(D(x))dx}+\\int_{z}{p_z(z)\\log(1-D(g(z)))dz} \\\\ =\\int_x[p_d(x)\\log(D(x))+p_g(x)\\log(1-D(x))]dx$$ 而一個 function \\(f(x)=a\\log (y)+b\\log (1-y)\\) 的最佳解為 \\(y=\\frac{a}{a+b}\\)因此我們得到 \\( D_G^*(x) = \\frac{p_d(x)}{p_d(x)+p_g(x)} \\) 步驟 B: $$\\begin{align*} &amp; C(G)=\\max_{D}V(G,D) \\\\ &amp; =E_{x \\sim p_d}[\\log D_G^*(x)]+E_{z \\sim p_z}[\\log(1-D_G^*(G(z)))] \\\\ &amp; =E_{x \\sim p_d}[\\log D_G^*(x)]+E_{x \\sim p_g}[\\log(1-D_G^*(x))] \\\\ &amp; =E_{x \\sim p_d}[\\log{\\frac{p_d(x)}{p_d(x)+p_g(x)}}]+E_{x \\sim p_g}[\\log{\\frac{p_g(x)}{p_d(x)+p_g(x)}}] \\end{align*}$$ 然後我們特別觀察如果 \\(p_g = p_d\\)，上式會 $$\\begin{align} =E_{x \\sim p_d}[-\\log 2]+E_{x \\sim p_g}[-\\log 2]=-\\log4 \\end{align}$$ 重新改寫一下 \\(C(G)\\) 如下 $$\\begin{align} C(G)=-\\log4+KL(p_d\\vert\\frac{p_d+p_g}{2})+KL(p_g\\vert\\frac{p_d+p_g}{2}) \\\\ =-\\log4+2JSD(p_d \\vert p_g) \\end{align}$$ 馬上觀察到 \\(JSD\\geq0\\) 和 \\(JSD=0 \\Leftrightarrow p_g = p_d \\)這表示 \\(C(G)\\) 最佳值為 \\(-\\log4\\)，且我們已知當 \\(p_g = p_d\\) 時達到最佳值 (式(4))，因此為最佳解 結論整個 GAN 的流程:我們基於一個生成器 \\(G\\) 去最佳化 \\(D\\) 得到 \\(D_G^*\\)，接著要繼續最佳化生成器的時候，問題從目標函式 (3) 變成等價於要最佳化一個 JSD 的問題 (式(5))。藉由最佳化 JSD 問題，得到新的 \\(G\\)，然後重複上面步驟，最後達到式(3)的最佳解，而我們可以保證此時生成器鍊成， \\(p_g = p_d\\)。 問題出在哪? 問題就出在最佳化一個 JSD 的問題上面 ! * JSD 有什麼問題?我們通過最佳化 JSD，而將 \\(p_g\\) 逐漸拉向 \\(p_d\\)。但是 JSD 有兩個主要的問題: A. 在 實際狀況 下，無法給初連續的距離值，導致 gradient 大部分都是 0，因而非常難以訓練B. 產生的樣本多樣性不足，collapse mode。 這邊要解釋一下 實際狀況 是什麼意思。一般來說，真實資料我們都會用非常高的維度去表示，然而資料的變化通常只被少數幾種變因所控制，也就是只存在高維空間中的 local manifold。例如一個 swiss roll 雖然是在 3 維空間中，但它是在一個 2 維的 manifold 空間裡。 這樣會造成一個問題就是， \\(p_d\\) 和 \\(p_g\\)，不會有交集，又或者交集處的集合測度為0!這樣的情況在JSD衡量兩個機率分布的時候會悲劇。作者給出了下面一個簡單易懂的例子: 兩個機率分布都是在一個 1 維的 manifold 直線上，x 軸的距離維 \\(\\theta\\)，此時的 JSD 值為右圖所示，全部都是 \\(\\log2\\)，除了在 \\(\\theta\\) 那點的值是 0 (pdf完全重疊)。這樣計算出的 Gradients 幾乎都是 0，這也就是為什麼 GAN 很難訓練的原因。 這問題在 WGAN 之前還是有人提出解決的方法，不過就很偏工程思考: 加入 noise 使得兩個機率分部有不可忽略的重疊。因此讓 GAN 先動起來，動起來之後，再慢慢地把 noise 程度下降。這是聰明工程師的厲害辦法! 但終歸來說還是治標。真正的治本方法，必須要替換掉 JSD 這樣的量測函式才可以。 本篇鋪梗結束 (這梗也太長了)。下篇終於輪到主角登場， WGAN 的 W !","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"generative model","slug":"generative-model","permalink":"http://yoursite.com/tags/generative-model/"}]},{"title":"WGAN Part 2: 主角 W 登場","date":"2017-03-16T13:25:12.000Z","path":"2017/03/16/WGAN-Part-2/","text":"前情提要GAN 作者設計出一個 Minimax game，讓兩個 players: 生成器 G 和 鑑別器 D 去彼此競爭，並且達到平衡點時，此問題達到最佳解且生成器 G 鍊成。大致上訓練流程為先 optimize 鑑別器 D for some iterations，然後換 optimize 生成器 G (在 optimize G 時，此問題等價於最佳化 JSD 距離)，重複上述 loop 直到達到最佳解。但是仔細看看原來的最佳化問題之設計，我們知道在最佳化 G 的時候，等價於最佳化一個 JSD 距離，而 JSD 在遇到真實資料的時會很悲劇。怎麼悲劇呢? 原因是真實資料都存在 local manifold 中，造成 training data 的 p.d.f. 和 生成器的 p.d.f. 彼此之間無交集 (或交集的測度為0)，在這種狀況 JSD = log2 (constant) almost every where。也因此造成 gradients = 0。這是 GAN 很難訓練的一個主因。 也因此 WGAN 的主要治本方式就是換掉 JSD，改用 Wasserstein (Earth-Mover) distance，而修改過後的演算法也是簡單得驚人! Wasserstein (Earth-Mover) distance我們先給定義後，再用作者論文上的範例解釋定義如下:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_g)=\\inf_{\\gamma\\in\\prod(\\mathbb{P}_r,\\mathbb{P}_g)}E_{(x,y)\\sim \\gamma}[\\Vert x-y \\Vert] \\end{align}$$\\(\\gamma\\)指的是 real data and fake data 的 joint distribution，其中 marginal 為各自兩個 distributions。先別被這些符號嚇到，直觀的解釋為: EM 距離可以理解為將某個機率分佈搬到另一個機率分佈，所要花的最小力氣。 我們用下面這個例子明確舉例，假設我們有兩個機率分佈 f1 and f2:$$\\begin{align*} f_1(a)=f_1(b)=f_1(c)=1/3 \\\\\\\\ f_1(A)=f_1(B)=f_1(C)=1/3 \\end{align*}$$這兩個機率分佈在一個 2 維平面，如下:而兩個 \\(\\gamma\\) 對應到兩種 搬運配對法$$\\begin{align*} \\gamma_1(a,A)=\\gamma_1(b,B)=\\gamma_1(c,C)=1/3 \\\\\\\\ \\gamma_2(a,B)=\\gamma_2(b,C)=\\gamma_2(c,A)=1/3 \\end{align*}$$可以很容易知道它們的 marginal distributions 正好符合 f1 and f2 的機率分佈。則這兩種搬運法造成的 EM distance 分別如下:$$\\begin{align*} EM_{\\gamma_1}=\\gamma_1(a,A)*\\Vert a-A \\Vert + \\gamma_1(b,B)*\\Vert b-B \\Vert + \\gamma_1(c,C)*\\Vert c-C \\Vert \\\\\\\\ EM_{\\gamma_2}=\\gamma_2(a,B)*\\Vert a-B \\Vert + \\gamma_2(b,C)*\\Vert b-C \\Vert + \\gamma_2(c,A)*\\Vert c-A \\Vert \\end{align*}$$明顯知道 $\\theta=EM_{\\gamma_1}&lt;EM_{\\gamma_2}$而 EM distance 就是在算所有搬運法中，最小的那個，並將那最小的 cost 定義為此兩機率分佈的距離。這個距離如果是兩條平行 1 維的直線 pdf (上面的例子是直線上只有三個離散資料點)，會有如下的 cost: 對比此圖和上一篇的 JSD 的結果，EM 能夠正確估算兩個沒有交集的機率分佈的距離，直接的結果就是 gradient 連續且可微 ! 使得 WGAN 訓練上穩定非常多。 一個關鍵的好性質: Wasserstein (Earth-Mover) distance 處處連續可微原始 EM distance 的定義 (式(1)) 是 intractable一個神奇的數學公式 (Kantorovich-Rubinstein duality) 將 EM distance 轉換如下:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_\\theta)=\\sup_{\\Vert f \\Vert _L \\leq 1}{ E_{x \\sim \\mathbb{P}_r}[f(x)] - E_{x \\sim \\mathbb{P}_\\theta}[f(x)] } \\end{align}$$注意到 sup 是針對所有滿足 1-Lipschitz 的 functions f，如果改成滿足 K-Lipschitz 的 functions，則值會相差一個 scale K。但是在實作上我們都使用一個 family of functions，例如使用所有二次式的 functions，或是 Mixture of Gaussians，等等。而經過近幾年深度學習的發展後，我們可以相信，使用 DNN 當作 family of functions 是很洽當的選擇，因此假定我們的 NN 所有參數為 \\(W\\)，則上式可以表達成:$$\\begin{align} W(\\mathbb{P}_r,\\mathbb{P}_\\theta)\\approx\\max_{w\\in W}{ E_{x \\sim \\mathbb{P}_r}[f_w(x)] - E_{z \\sim p(z)}[f_w(g_{\\theta}(z))] } \\end{align}$$這裡不再是等式，而是逼近，不過 Deep Learning 優異的 Regression 能力是可以很好地逼近的。 我們還是需要保證整個 EM distance 保持處處連續可微分，否則就面臨跟 JSD 一樣的窘境，針對這點，WGAN 作者很強大地證明完了，得到結論如下: 針對生成器 \\(g_\\theta\\)任何 feed-forward NN 皆可 針對鑑別器 \\(f_w\\)當 \\(W\\) 是 compact set 時，該 family of functions \\(\\{f_w\\}\\) 滿足 K-Lipschitz for some K。具體實現很容易，因為在 \\(R^d\\) space，compact set 等價於 closed and bounded，因此只需要針對所有的參數取 bounding box即可!論文裡使用了 [-0.01,0.01] 這個範圍做 clipping。 與 GAN 第一個不同點為: 鑑別器參數取 clipping。 EM distance 為目標函式所造成的不同我們將兩者的目標函式列出來做個比較$$\\begin{align} GAN: E_{x \\sim \\mathbb{P}_r} [\\log f_w(x)] + E_{z \\sim p(z)}[\\log (1-f_w(g_{\\theta}(z)))] \\\\ WGAN: E_{x \\sim \\mathbb{P}_r}[f_w(x)] - E_{z \\sim p(z)}[f_w(g_{\\theta}(z))] \\end{align}$$發現到 WGAN 不取 log，同時對生成器的目標函式也做了修改 與 GAN 第二個不同點為: WGAN 的目標函式不取 log，同時對生成器的目標函式也做了修改。 第三個不同點是作者實驗的發現 與 GAN 第三個不同點為: 使用 Momentum 類的演算法，如 Adam，會不穩定，因此使用 SGD or RMSProp。 WGAN 演算法總結一下與 GAN 的修改處 A. 鑑別器參數取 clipping。B. WGAN 的目標函式不取 log，同時對生成器的目標函式也做了修改。C. 使用 SGD or RMSProp。 WGAN 的優點一: 目標函式與訓練品質高度相關原始的 GAN 沒有這樣的評量指標，因此會在訓練中途用人眼去檢查訓練是否整個壞掉了。 WGAN 解決了這個麻煩。作者的範例如下，可以發現WGAN的目標函式 Loss 愈低，sampling出來的品質愈高。 二: 鑑別器可以直接訓練到最好原始的 GAN 需要小心訓練，不能一下子把鑑別器訓練太強導致導函數壞掉 三: 不需要特別設計 NN 的架構GNN 使用 MLP (Fully connected layers) 難以訓練，較成功的都是 CNN 架構，並搭配 batch normalization。而在 WGAN 演算法下， MLP架構可能穩定訓練 (雖然品質有下降) 四: 沒有 collapse mode (保持生成多樣性)作者自己說在多次實驗的過程都沒有發現這種現象 Tensorflow 範例測試… TODO","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"generative model","slug":"generative-model","permalink":"http://yoursite.com/tags/generative-model/"}]},{"title":"Why-Aggregation-Work","date":"2017-03-13T13:29:47.000Z","path":"2017/03/13/Why-Aggregation-Work/","text":"為何三個臭皮匠會勝過一個諸葛亮?在 ML 中有一類的演算法稱為 Aggregation Methods，這方法的運作方式其實我們可能從小就接觸到了。有沒有遇過一種情況就是，當一群人遇到一個不知道最好答案的時候，最直接的方式就是大家的答案取平均。聽起來很直覺，但心裡老覺得怪怪的，因為根本不知道到底可不可靠。Aggregation methods 就是這樣的運作模式，這邊就給個結論，它很可靠! 以下的推導出自於林軒田教授的講義，這裡用自己的理解方式重新表達，主要作筆記用 開頭還是給先定義清楚一些 terms，對於理解式子才不會混淆 定義在先 Input: \\(x \\in X\\) 正確答案: \\(f(x)\\) 臭皮匠: \\(g_t(x),t=1,2,…\\) 臭皮匠們的決策結果: \\(G(x)=avg_t(g_t(x))\\) 衡量方法 \\(g\\) 的錯誤率: \\( Error(g)=E_x[(g(x)-f(x))^2]\\) 這邊要特別說的是衡量一個方法 \\(g\\) 的錯誤率，是針對所有的 input \\(x\\)，也就是針對 \\(X\\) domain 來算期望平方誤差 運算簡單但有點神奇的推導 我們先針對 一個固定的 x，來看看臭皮匠們統合的意見是否真的會得到較好的結果，由於input已經固定，所以下面會忽略 x 的 term首先是 “臭皮匠們各自的平方錯誤率” 的平均值$$avg_t((g_t-f)^2)$$將平方拆開後得$$=avg_t(g_t^2-2g_tf+f^2)$$將 avg 移入並用 G=avg(gt) 定義得到$$=avg_t(g_t^2)-2Gf+f^2$$再做如下的簡單代數運算$$=avg_t(g_t^2)-G^2+(G-f)^2 \\\\=avg_t(g_t^2)-2G^2+G^2+(G-f)^2 \\\\=avg_t(g_t^2-2g_tG+G^2)+(G-f)^2 \\\\=avg_t((g_t-G)^2)+(G-f)^2$$ 目前為止是針對 一個特定的輸入 x，而我們需要知道的是對 整個 domain X 的錯誤率因此真正要計算的是這個目標錯誤率$$avg_t(Error(g_t))=avg_t(E_x[(g_t(x)-f(x))^2])$$將 Expection for all x 代入進去剛剛上面針對一個 x 的結果，得到如下式子\\begin{eqnarray}=avg_t(E_x[(g_t(x)-G(x))^2])+E_x[(G(x)-f(x))^2] \\\\=avg_t(E_x[(g_t(x)-G(x))^2])+Error(G) \\\\\\geq Error(G) \\end{eqnarray} 怎麼解釋?重複一下最後的重要式子: $$avg_t(Error(g_t)) = avg_t(E_x[(g_t(x)-G(x))^2])+Error(G) \\\\\\geq Error(G)$$ 最直接的結論就是: “統合出來的結果”的錯誤率 會比 “各自決定”的平均錯誤率 還要低 可以看到針對 一組固定 的臭皮匠們 \\({g_t}\\)，不等式左邊 \\(avg_t(Error(g_t))\\) 是固定值，因此若要找一個統合大家意見的方法 \\(G\\)，而該方法有最小的錯誤率 (最小化 \\(Error(G)\\) )，很明顯就是要最大化 \\(avg_t(E_x(g_t-G)^2)\\)，而此最大化的結果 就是 \\(G\\) 是 \\({g_t}\\) 的平均值(uniform blending)，符合我們一開始說的最直覺的策略! 另一方面，如果我們選到兩組 set \\({g_t}\\) and \\({h_t}\\) 他們的 Error 相同: \\(avg_t(Error(g_t))= avg_t(Error(h_t))\\) ，那我們當然是要選擇意見最不同的那一組臭皮匠們，這是因為意見愈不同代表 \\(avg_t(E_x(g_t-G)^2)\\) 愈大，因而導致 \\(Error(G)\\) 會愈小。 小結 剛剛上面這個結論就很有趣，意見遇不同的話，統合起來的效果愈好，也就是你我之間的意見有很大的分歧時，這代表是好事! 事實上 Adaboost 就是採取這麼一個策略，每一次的 iteration 會選擇跟上次統合完的結果意見差最多那一位臭皮匠進來，有機會再補上 Adaboost，這是我很喜歡的一種 ML 演算法。 而這邊還可以引出一個方法, Bootstrap. Bootstrap aggregation方法很簡單。對我們的dataset每一次重新resampling (e.g. 取N’筆，每次取的data都再放回去，因此data可以重複。可重複這點造成dataset的point具有weight的性質，這在adaboost每一次iteration的re-weighting有同樣意思) 這個叫做bootstrap，針對該次的data算出我們的weak learner gt，iterate很多次後，把每一次的gt做uniform blending。 我認為 aggregation methods 就算放到現在的 Deep Learning 火熱的時代還是相當有用的，除了本身這些方法如 adaboost 好用之外，其概念也相當有用，例如 Deep Learning 的 dropout 事實上可以用 bootstrap 來解釋 (有機會再補上資料)","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"uniform blending","slug":"uniform-blending","permalink":"http://yoursite.com/tags/uniform-blending/"},{"name":"aggregation","slug":"aggregation","permalink":"http://yoursite.com/tags/aggregation/"},{"name":"adaboost","slug":"adaboost","permalink":"http://yoursite.com/tags/adaboost/"},{"name":"bootstrap","slug":"bootstrap","permalink":"http://yoursite.com/tags/bootstrap/"}]},{"title":"Vehicle-Tracking","date":"2017-03-12T14:27:13.000Z","path":"2017/03/12/Vehicle-Tracking/","text":"這個 Porject 目的是要偵測畫面中所有的車子, 大致上的流程是先訓練好 car/non-car 的 classifer, 然後用 sliding window 搭配不同的 window size 去偵測, 最後再把 bounding boxes 做一些後處理, 例如 merge boxes, 和對時間序列的處理以下為 git hub 的 REAMDE.md The goals / steps of this project are the following: Perform a Histogram of Oriented Gradients (HOG) feature extraction I implement HOG feature extraction and using a subset of training data to search a good settings of parameters. Images are stored in output_images/HOG_with_YCrCb.jpg and output_images/grid_search.jpg Train Classifier I trained a Linear SVM classifier with HOG + color_hist + bin_spatial which achieved 98% accuracy on test set. Sliding Window Search I implemented a sliding window search method with two scales of window. HOG features are extracted once for an given image. Showing Examples so far I showed 4 examples with the pipeline so far. Image is stored in output_images/example_before_post_processing.jpg Video Implementation I showed the results with a short video clip (test_video.mp4) as well as the final result that adopted post-processing below. Further Post-processing A buffer for heat-maps is used for keeping a 6 consecutive heat-maps in frames. This will filtered out some false accepts. Discussion A short discussion is made. – Rubric Points 1. Histogram of Oriented Gradients (HOG) Explain how (and identify where in your code) you extracted HOG features from the training images. Explain how you settled on your final choice of HOG parameters. I randomly selected examples of car and notcar and showed their HOG results in each channel of HLS space: In order to get a good enough setting for those parameters (orientations, pixels_per_cell and cells_per_block), I applied a grid searching method with a linear SVM on a small subset of training data. Grid searching space is defined as follows (24 combinations): 123orient_set = range(9,19,3)pix_per_cell_set = [4,8,16]cell_per_block_set = [1,2] The purpose of this stage is not finding the optimal, but rather, a good enough setting. So I choose orient=15, pix_per_cell=8, cell_per_block=2, cspace=&#39;RGB2YCrCb&#39; 2. Train Classifier Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them). Before training the classifier, dataset should be processed first.Since the vehicles/GTI*/*.png contains time-series data, I manually selected images to avoid train and test sets having identical images. In addition, 20% images in each training folder are treated as test images. The same partition method applied to non-vehicles images too. Then I trianed a Linear SVM model with HOG + color_hist + bin_spatial features which has performance: 1inside-acc=1.0, outside-acc=0.9802036199095022 3. Sliding Window Search Describe how (and identify where in your code) you implemented a sliding window search. How did you decide what scales to search and how much to overlap windows? The course provided a very useful code snippet that can extract HOG features once no matter how much windows are. So I reuse it as the feature extraction function!I used two types of scales, 1.5 and 1.2, which deal with large and small window respectively (car with near and far positions from camera). Also, I found that the overlaping of cells_per_step = 1 (more dense windows) has better results in my implementation. Before going through, it is worth checking the image values. Since feature extraction pipeline processed .png files with mpimg.imread, it reads images with values [0,1]. However, mpimg.imread reads the .jpg file with values within [0,255]. So it is necessary to divide 255 before calling the feature extraction pipeline while reading .jpg images with mpimg.imread. Make sure your images are scaled correctly The training dataset provided for this project ( vehicle and non-vehicle images) are in the .png format. Somewhat confusingly, matplotlib image will read these in on a scale of 0 to 1, but cv2.imread() will scale them from 0 to 255. Be sure if you are switching between cv2.imread() and matplotlib image for reading images that you scale them appropriately! Otherwise your feature vectors can get screwed up. To add to the confusion, matplotlib image will read .jpg images in on a scale of 0 to 255 so if you are testing your pipeline on .jpg images remember to scale them accordingly. And if you take an image that is scaled from 0 to 1 and change color spaces using cv2.cvtColor() you’ll get back an image scaled from 0 to 255. So just be sure to be consistent between your training data features and inference features! 4. Showing Examples Show some examples of test images to demonstrate how your pipeline is working. How did you optimize the performance of your classifier? The followings are some examples. As you can see in the example 2, there exists a false accept. This will be filtered out in the post-processing part. 5. Video Implementation Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.) Following is the final result (combined with post-processing as described below) 6. Further Post-processing Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes. A heat-map to further filtered out some false positives. Moreover, I used a buffer to keep the 6 consecutive frames of heat-maps, and then accumulated those heat-maps in buffer. The accumulated heat-map then thresholded and produced the final results. 7. Discussion Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust? There still have too much parameters that effect the robustness, like ystart, ystop, scale factors, thresholds for heat-maps, and etc. Moreover, with more challanging conditions, those settings might work in one condition but fail in others. I think the most important part in those pipelines is the classifier itself. The linear SVM I used in this project is not good enough as you can see in the video that still has few false accepts. So a deep-learning based classifier might achieve better results and actually helpful to the following pipelines. This would be my future work.","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"CV","slug":"CV","permalink":"http://yoursite.com/tags/CV/"}]},{"title":"Lane-Finding","date":"2017-02-27T02:12:28.000Z","path":"2017/02/27/Lane-Finding/","text":"以下是 github 上的 README, 全英文. 此 Project 主要都是在做 Computer Vision 相關的東西. 學到了許多使用 Python and CV 相關的技巧. 整理來說是個滿有趣的 project! The goals / steps of this project are the following: Compute the camera calibration matrix and distortion coefficients given a set of chessboard images. Apply a distortion correction to raw images. Use color transforms, gradients, etc., to create a thresholded binary image. Apply a perspective transform to rectify binary image (“birds-eye view”). Detect lane pixels and fit to find the lane boundary. Determine the curvature of the lane and vehicle position with respect to center. Warp the detected lane boundaries back onto the original image. Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position. Rubric Points1. Camera calibrationThe images for calculating the distortion and 3-D to 2-D mapping matrix are stored in ./camera_cal/calibration*.jpg.Firstly, I used cv2.findChessboardCorners to find out all those corner points (corners) in the images.Then I used cv2.calibrateCamera to calculate the distortion (dist) and mapping matrix (mtx) given the corners pts and their corresponding predifined 3-D pts objp 2. Provide an example of a distortion-corrected imageHere is an example of distortion-corrected image: 3. Create a thresholded binary image and provide exampleI used magnitude of gradients, direction of gradients, and L and S in HLS color space.A combined rule is used: 12combined[((mag_binary == 1) &amp; (dir_binary == 1)) |\\ ((hls_binary == 1) &amp; (dir_binary == 1) &amp; (bright_binary == 1))] = 1 Example masking image is showed: Moreover, I used widgets to help tunning the parameters of those masking functions. It can provide instantaneous binary result that really help for accelarating this step. The widgets codes are list here: 123456789def interactive_mask(ksize, mag_low, mag_high, dir_low, dir_high, hls_low, hls_high, bright_low, bright_high): combined = combined_binary_mask(image,ksize, mag_low, mag_high, dir_low, dir_high,\\ hls_low, hls_high, bright_low, bright_high) plt.figure(figsize=(10,10)) plt.imshow(combined,cmap='gray') interact(interactive_mask, ksize=(1,31,2), mag_low=(0,255), mag_high=(0,255),\\ dir_low=(0, np.pi/2), dir_high=(0, np.pi/2), hls_low=(0,255),\\ hls_high=(0,255), bright_low=(0,255), bright_high=(0,255)) 4. Perspective transformFirst, I defined the source and destination of perspective points as follows: Source Destination 585, 460 320, 0 203, 720 320, 720 1127, 720 960, 720 695, 460 960, 0 Then the perspective_warper function is defined which returns perspective image and the matrix warpM as well.warM is needed for the later step which does the inverse perspective back to the original image. 1perspective_img, warpM = perspective_warper(undist,src,dst) An example is showed here: 5. Lane line pixel and polynomial fittingI applied a windowing approach to identify the lane pixels In this example, I used 9 windows for both lane lines. The window is processed in an order from the buttom to the top. Pixels are detected by the following function 1def identify_lane_pixel(img, lcenter_in, rcenter_in, win_num=9, win_half_width=150, start_from_button=False): lcenter_in and rcenter_inare the centers (in horizontal coordinate) of windows. win_num defines how many windows are used. In this example, 9. win_half_width refers to the half length of window width start_from_button indicates how the initial centers of windows are set. Specifically, Let the current window as j and current frame index as i. If start_from_button=True, the center of window j will be initally set as window j-1. Otherwise, it will be initally set as window j in frame i-1. Then, by using the initial position just set, the lane pixels are identified if the histogram of that window is high enough. Finally, based on those identified pixels, update the center position of current widnow j. Next, a simple second order polynomial fitting is applied to both identified pixels 123# Fit a second order polynomial to eachleft_fit = np.polyfit(lpixely, lpixelx, 2)right_fit = np.polyfit(rpixely, rpixelx, 2) But wait! Since we are assuming “birds-eye view”, both lanes should be parallel! So I first tried a method that ties the polynomial coefficients except the shifting ones! this method results in the following example As can be seen in the figure, curves are indeed parallel. However, when I applied this method to the final video, I found that it wobbling a lot! (see “8. Video” below) After some investigation, I wonder that this problem is caused by the fixed source points of perspective. Since the pre-defined source points are always at the center of the camera while the lane curves are usually not, the result perspective curves is intrinsically not parellel! Hence, I applied a dynamic source point correction. Idea of method is showed in the follows: mapping inversely from coordinates in perspective images to original images can use the following formula: and results in the following example It works great! Unfortunately, if the lane curves are not stable, the resulting new source points may fail. This is the major difficulty of this method! (see “8. Video” below) 6. Radius of curvature of the lane and the position of the vehicleThe curvature is calculated based on the following formula. Udacity provides a very good tutorial here ! 1234a1, b1, c1 = left_fit_coefficientsa2, b2, c2 = right_fit_coefficientsr1 = ((1+(2*a1*height*ym_per_pix+b1)**2)**1.5)/(2*np.abs(a1))r2 = ((1+(2*a2*height*ym_per_pix+b2)**2)**1.5)/(2*np.abs(a2)) There’s no need to worry about absolute accuracy in this case, but your results should be “order of magnitude” correct. So I divide my result by 10 to make it seems more reasonable. And of course, the “order of magnitude” remains intact. 7. Warp the detected lane boundaries back onto the original imageIn order to warp back onto the original image, we need to calculate the inverse of perspective transform matrix warpMjust apply Minv = inv(warpM) which is from numpy.linalg import inv Then, simply apply cv2.warpPerspective with Minv as input. Note: use cv2.putText to print the curvature and position onto images 8. Video Simple poly-fit (Most stable! Simple is better ?!) Shared coefficients of poly-fit (Wobbling problem) Dynamic source points of perspective (Unstable, crash sometimes. If the lane curves are not stable, the resulting new source points may fail) DiscussionBasically, I applied those techniques suggested by Udacity. I did some efforts trying to parallize both curves in the perspective “bird eye view”. Two methods are applied Shared coefficients of polynomial fitting Dynamic source points of perspetive Each has its own issue. For (1.), wobbling, and for (2.) unstable. Future works will focus on solving the (2.) unstable issue. Maybe a smoothing method is a good idea. Moreover, for more difficult videos, pixels may not be detected which makes the pipeline crash. One way to overcome this problem is when this issue happens, the lane curve is set to be the same as previous frame. Generelizing this idea, a confidence measure of lane pixels is worth to apply. If the confidence is low, then set the lane curve as the same as previous frame might be a good way to better estimate result. Finally, finding a robust combination of masking rule and tweaking those parameters precisely might help too. 附上中文其他討論: Reviewer 給了很多有用的 article links! 這邊附上做未來參考 Perspective bird eye view:http://www.ijser.org/researchpaper%5CA-Simple-Birds-Eye-View-Transformation-Technique.pdfhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3355419/https://pdfs.semanticscholar.org/4964/9006f2d643c0fb613db4167f9e49462546dc.pdfhttps://pdfs.semanticscholar.org/4074/183ce3b303ac4bb879af8d400a71e27e4f0b.pdf Lane line pixel identification:https://www.researchgate.net/publication/257291768_A_Much_Advanced_and_Efficient_Lane_Detection_Algorithm_for_Intelligent_Highway_Safetyhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC5017478/https://chatbotslife.com/robust-lane-finding-using-advanced-computer-vision-techniques-46875bb3c8aa#.l2uxq26sn lane detection with deep learning:http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w3/papers/Gurghian_DeepLanes_End-To-End_Lane_CVPR_2016_paper.pdfhttp://lmb.informatik.uni-freiburg.de/Publications/2016/OB16b/oliveira16iros.pdfhttp://link.springer.com/chapter/10.1007/978-3-319-12637-1_57 (chapter in the book Neural Information Processing)http://ocean.kisti.re.kr/downfile/volume/ieek1/OBDDBE/2016/v11n3/OBDDBE_2016_v11n3_163.pdf (in Korean, but some interesting insights can be found from illustrations)https://github.com/kjw0612/awesome-deep-vision (can be useful in project 5 - vehicle detection)噁心到吐血的真實挑戰: 還是老話一句, 真的要成為可用的產品, 難道超級無敵高阿!!","tags":[{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"},{"name":"CV","slug":"CV","permalink":"http://yoursite.com/tags/CV/"}]},{"title":"Neural Art","date":"2017-02-13T14:04:36.000Z","path":"2017/02/13/Neural-Art/","text":"Art with Neural Network風格, 創作這種能力在現在Alpha Go已經稱霸的時代, 目前覺得還是人類獨有的不過有趣的是, 對於那些已經在 ImageNet 訓練得非常好的模型, 如: VGG-19, 我們通常已經同意模型可以辨別一些較抽象的概念那麼是否模型裡, 也有具備類似風格和創作的元素呢? 又或者風格在模型裡該怎麼表達? 本篇文章主要是介紹這篇 A Neural Algorithm of Artistic Style 的概念和實作, 另外一個很好的投影片 by Mark Chang 也很值得參考 先給出範例結果, 結果 = 原始的內容 + 希望的風格 Content Image Style Image Result Image 說在前頭的最佳化在講下去之前, 我們先講 NN 的事情, 一般情況, 我們是給定 input image x, 而參數 w 則是要求的變數, 同時對 loss (objective function) 做 optimize, 實作上就是 backprob.上面講到的三種東西列出來: x: input image (given, constant) w: NN parameters (variables) loss: objective function which is correlated to some desired measure 事實上, backprob 的計算 x and w 角色可以互換. 也就是將 w 固定為 constant, 而 x 變成 variables, 如此一來, 我們一樣可以用 backprob 去計算出最佳的 image x.因此, 如果我們能將 loss 定義得與風格和內容高度相關, 那麼求得的最佳 image x 就會有原始的內容和希望的風格了!那麼再來就很明確了, 我們要定義出什麼是 Content Loss 和 Style Loss 了 Content Loss針對一個已經訓練好的 model, 我們常常將它拿來做 feature extraction. 例如一個 DNN 把它最後一層辨識的 softmax 層拿掉, 而它的前一層的 response (做forward的結果), 就會是對於原始 input 的一種 encoding. 理論上也會有很好的鑑別力 (因最只差最後一層的softmax). Udacity 的 traffic-sign detection 也有拿 VGG-19, ResNet, 和 gooLeNet 做 feature extraction, 然後只訓練重新加上的 softmax layer 來得到很高的辨識率. 因此, 我們可以將 forward 的 response image 當作是一種 measure content 的指標!知道這個理由後, 原文公式就很好理解, 引用如下: So let p and x be the original image and the image that is generated and Pl and Fl their respective feature representation in layer l. We then define the squared-error loss between the two feature representations 簡單來說 Pl 是 content image P 在 l 層的 response, 而 Fl 是 input image x (記得嗎? 它是變數喔) 在 l 層的 response.這兩個 responses 的 squared-error 定義為 content loss, 要愈小愈好. 由於 response 為 input 的某種 encoded feature, 所以它們如果愈接近, input 就會愈接近了 (content就愈接近).引用 Mark Chang 的投影片: Style Loss個人覺得最神奇的地方就在這裡了! 當時自己怎麼猜測都沒猜到可以這麼 formulate.我個人的理解是基於 CNN 來解釋假設對於某一層 ConvNet 的 kernel 為 w*h*k (width, hieght, depth), ConvNet 的 k 通常代表了有幾種 feature maps說白一點, 有 k 種 filter responses 的結果, 例如第一種是線條類的response, 第二種是弧形類的responses … 等等而風格就是這些 responses 的 correlation matrix! (實際上用 Gram matrix, 但意義類似)基於我們對於 CNN 的理解, 愈後面的 layers 能處理愈抽象的概念, 因此愈後面的 Gram matrix 也就愈能代表抽象的 style 概念.原文公式引用如下: 總之就是計算在 l 層上, sytle image a 和 input image x 它們的 Gram matrix 的 L2-norm 值 一樣再一次引用 Mark Chang 的投影片:也可以去看看他的投影片, 有不同角度的解釋 實戰主要參考此 gitHub一開始 load VGG-19 model 就不說了, 主要的兩個 loss, codes 如下:123456789101112131415161718def content_loss_func(sess, model): \"\"\" Content loss function as defined in the paper. \"\"\" def _content_loss(p, x): # N is the number of filters (at layer l). N = p.shape[3] # M is the height times the width of the feature map (at layer l). M = p.shape[1] * p.shape[2] # Interestingly, the paper uses this form instead: # # 0.5 * tf.reduce_sum(tf.pow(x - p, 2)) # # But this form is very slow in \"painting\" and thus could be missing # out some constants (from what I see in other source code), so I'll # replicate the same normalization constant as used in style loss. return (1 / (4 * N * M)) * tf.reduce_sum(tf.pow(x - p, 2)) return _content_loss(sess.run(model['conv4_2']), model['conv4_2']) 12345678910111213141516171819202122232425262728293031323334353637383940414243# Layers to use. We will use these layers as advised in the paper.# To have softer features, increase the weight of the higher layers# (conv5_1) and decrease the weight of the lower layers (conv1_1).# To have harder features, decrease the weight of the higher layers# (conv5_1) and increase the weight of the lower layers (conv1_1).STYLE_LAYERS = [ ('conv1_1', 0.5), ('conv2_1', 1.0), ('conv3_1', 1.5), ('conv4_1', 3.0), ('conv5_1', 4.0),]def style_loss_func(sess, model): \"\"\" Style loss function as defined in the paper. \"\"\" def _gram_matrix(F, N, M): \"\"\" The gram matrix G. \"\"\" Ft = tf.reshape(F, (M, N)) return tf.matmul(tf.transpose(Ft), Ft) def _style_loss(a, x): \"\"\" The style loss calculation. \"\"\" # N is the number of filters (at layer l). N = a.shape[3] # M is the height times the width of the feature map (at layer l). M = a.shape[1] * a.shape[2] # A is the style representation of the original image (at layer l). A = _gram_matrix(a, N, M) # G is the style representation of the generated image (at layer l). G = _gram_matrix(x, N, M) result = (1 / (4 * N**2 * M**2)) * tf.reduce_sum(tf.pow(G - A, 2)) return result E = [_style_loss(sess.run(model[layer_name]), model[layer_name]) for layer_name, _ in STYLE_LAYERS] W = [w for _, w in STYLE_LAYERS] loss = sum([W[l] * E[l] for l in range(len(STYLE_LAYERS))]) return loss 一開始給定 random input image:style image 選定如下:隨著 iteration 增加會像這樣: 第一次的 backprob: 1000 iteration: 2000 iteration: 3000 iteration: 4000 iteration: 5000 iteration: 短節這之間很多參數可以調整去玩, 有興趣可以自己下載 gitHub 去測 上一篇的 “GTX 1070 參見” 有提到, 原來用 CPU 去計算, 1000 iteration 花了六個小時! 但是強大的 GTX 1070 只需要 6 分鐘! 不過, 就算是給手機用上GTX1070好了 (哈哈當然不可能), 6分鐘的一個結果也是無法接受!PRISMA 可以在一分鐘內處理完! 這必定不是這種要算 optimization 的方法可以達到的.事實上, 李飛飛的團隊發表了一篇論文 “Perceptual Losses for Real-Time Style Transfer and Super-Resolution“訓練過後, 只需要做 forward propagation 即可! Standford University 的 JC Johnson 的 gitHub 有完整的 source code!找時間再來寫這篇心得文囉!","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Art","slug":"Art","permalink":"http://yoursite.com/tags/Art/"}]},{"title":"GTX 1070","date":"2017-02-12T13:44:40.000Z","path":"2017/02/12/GTX-1070/","text":"NVIDIA GTX 1070 參見經過兩次的Udacity DNN Projects後, 我受不了用CPU訓練了! 這實在是太慢了!考量應該會長期使用GPU, AWS實在不怎麼便宜 (1hr=1USD @ Tokyo site), 加上local端訓練也比較方便, 就殺下去了!! 安裝 CUDA and cuDNN大致上的安裝流程如下, 並不複雜, 更詳細可參考 link 安裝 CUDA Drivers上述聯結中有下載路徑, 然後照頁面一步步選擇 (Operating System, Version, Installer Type)Installer Type如果網路不好建議選擇 exe local, 然後下載後執行安裝就對了Windows 環境變量中 CUDA_PATH 是 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0, 但是仍須加上 bin\\ 和 lib\\x64\\, 記得加上. 安裝 cuDNN要下載這個還要填一些登入資料, 需要再Accelerated Computing Developer Program註冊, 總之註冊後就可下載解壓後會有一個資料夾 cuda, 裡面三個子資料夾 bin, include, lib將上述的檔案放到相對應的 CUDA Driver 的安裝路徑內, 預設是在 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0 安裝 tensorflow-gpu最簡單的一步pip install tensorflow-gpu然後即可測試, 如果有成功會有以下畫面, 注意 successfully 字有無出現 1234567&gt;&gt;&gt; import tensorflow as tfI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locallyI c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally&gt;&gt;&gt; 測試 GTX 1070 強大能力使用兩個極端例子分別測試有無使用GPU速度上的差異 Neural Art 的例子: A Neural Algorithm of Artistic Style 這個例子是所有的東西都可以 load 進 memory 中, 因此沒有任何 I/O, 直接比拚運算能力! 因此可以直接看出 GPU 和 CPU 的計算能力差異 結果: 時間沒有很嚴格計算, 是看產生結果的時間稍微計算的, 但這效能已經很誇裝了, 60倍, 60倍, 60倍!跑出來的圖: Content Image Style Image Result Image 不是所有的情況都能把 training data 和 model 都 load 進 memory 中, 所以勢必會有其他拖慢速度的環節, 其中最慢的就是 I/O 剛好 Udacity 的 project 3 就是每筆 training data 都需要去 load image 並且 on-the-fly 運算一堆 preprocessing. 這個情況剛好是另一種可能的極端 結果跑一個epoch所花的時間為這種case看來只能加速到約 2倍. 沒辦法, 其他拖油瓶的動作佔太多比例了 短結大部分的情況下, 提升的速度範圍會落在 2~60 倍 之間, 總之是值得的! 就算不玩DNN, 電動也要把它打到物超所值!","tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"NVIDIA","slug":"NVIDIA","permalink":"http://yoursite.com/tags/NVIDIA/"},{"name":"cuDNN","slug":"cuDNN","permalink":"http://yoursite.com/tags/cuDNN/"},{"name":"CUDA","slug":"CUDA","permalink":"http://yoursite.com/tags/CUDA/"}]},{"title":"Driving by Learning Your Style","date":"2017-02-05T13:58:07.000Z","path":"2017/02/05/Driving-by-Learning-Your-Style/","text":"Udacity Self Driving Project 3: behavioral cloningA great simulator is provided that can log your driving data (speed, throttle, brake, steering, and images) and test the driving algorithm.Two modes are provided, Training mode and Atuonomous mode. By using Training mode, you can collect training data to train the model. Then test the model with the Atuonomous mode. For those driving log data, steering and images are the most important features that we are going to use in this project. The goal is, given an image, find out the corresponding steering angle. Some might wonder that speed, throttle, and brake are features that are useful too.Also, driving images are time correlated, not just a given static image.With ignoring so much useful information, does the goal still reasonable? Nvidia just showed it works! and works pretty well!So our first step is to collect the data, and fortunately, Udacity provides data for us and I used it for training. Training Data Analysis8036 data are provided. Each data has 3 positions of images (left, center, right) with 1 corresponding steering angle.Most of angles are 0, and I found that randomly ignoring half of 0-angle data is fine and can speed up. Moreover, I duplicated some samples that has angles within the range +-[0.2, 1] in order to balance the data.Histograms of before/after data selection are shown below: Data AugmentationData augmentation is a practical way to avoid overfit and generalized the model. I used 5 types of augmentations: Flipping – Flipping is a useful way to balance both turns of data. For each data, a 1/2 probability is used to decide wheter to flip. Also, steering angle is multiplied by -1. Horizontal shift – [-20,+20] pixels are randomly selected as the shift value. By doing so, it can help to recover the vehicle when it goes outside the lane.By referencing this article, I added 0.004 steering angle units per pixel shift to the right, and subtracted 0.004 steering angle units per pixel shift to the left.Results in [-0.8~+0.8] steering values adjustment which corresponding to [-2~+2] degrees (steering value * 25 = degree) Brightness – Brightness is done in the “HSV” domain. I found that with a ratio of [0.5~1.1] for “V” domain works fine. Blurring – A Gaussian blur with kernel size 3 is applied. Not sure how useful of this method helps for robustness. Left/Right camera images – These left/right images are very useful for data augmentation and also help for recovering off-lane driving. Udacity: You also might wonder why there are three cameras on the car: center, left, and right. That’s because of the issue of recovering from being off-center.In the simulator, you can weave all over the road and turn recording on and off. In a real car, however, that’s not really possible. At least not legally.So in a real car, we’ll have multiple cameras on the vehicle, and we’ll map recovery paths from each camera. I adjusted the steering angles for left/right images with a naive method. Following figure shows how I correct the angle of right image: I found that setting offset = 6 or 5 is good enough. For large value, the car starts zig-zagging. An example of correction shows below, where the steering angles are indicated by red lines: Data Normalization Normalization – Images are normalized with (x-128)/128. Cropping – Images are trimmed with 40, 20, 20, and 20 pixels from top, bottom, left, and right respectively. This will cut most of the car hood and sky. Resizing – resized to 66 x 200, same as NVIDIA CNN. Model ArchitectureI adopted NVIDIA CNN with dropout layers: Generator and Training Generator: It is very useful to use a python generator to feed the training data batch-by-batch rather than loading all the data in memory at once.A useful link to learn python iterator/generator list here ( for those who doesn’t familiar with python just like me :) ). In order to further speed up. I tried pre-loading a chunck of data, e.g. 5000 images, into memory, and loaded another chunck if the batch data (required by generator) is outside the chunck in memory. However, it does not speed up! Somewhat weired. For each input images, a position is randomly chosen (left,center,right).Then flipping and shadowing are applied with a random fair coin. Finally, brighteness and horizonal shift are adopted with the corresponding angle adjustment. Training: Some hyper-parameters are listed: epoch–50 samples for each epoch – 8896 optimizer – Adam with 1e-4 batch-size – 64 Although Keras did shuffle, it only applies in the batched data. So I shuffled the entire training set for each epoch to get more de-correlated data. Driving PolicyI found that instead of giving a constant throttle, controlling to a constant speed is more stable to drive.So I used a simple policy that tries to keep speed near 20. 123456789speed = float(speed) if speed &gt; 25: throttle = 0.05 elif speed &gt; 20: throttle = 0.2 elif speed &gt; 10: throttle = 0.35 else: throttle = 0.5 ResultsSee below for the track1 drive. However, I failed on track2. Hit a wall during a right turn and still working on it.Hope some tweaks on data selection and model architecture might work~","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"}]},{"title":"traffic-sign-detection","date":"2017-01-18T14:35:21.000Z","path":"2017/01/18/traffic-sign-detection/","text":"前言終於來到 project 2 了, 這次的主要目的是練習使用 tensorflow 做交通號誌識別Dataset 為 German Traffic Sign Dataset有43種交通號誌, 是一種43選1的概念, 因為沒有考慮都不是這個選項, 理論上這類問題較簡單, 有researcher達到99.81%的辨識率 共 51839 張 training data, 而 testing 有 12630 張, 分佈如下, 可以看的出來資料分佈不均每種類別 random 挑一張出來如下圖Udacity 很好心的幫忙把所有的 image 幫你打包成只剩下 traffic sign Download, 且 cv2.resize(image,(32,32)) 了, 只需要 pickle.load 下來就搞定而原始的 data 是給你一大張image, 然後再告訴你那些traffic signs在image中的rectangular window座標, 還要再多處理較麻煩 要注意的一點是, dataset 是經由一秒鐘的 video 擷取下來, 因此鄰近的 data 會很相近 [1], 如果使用 train_test_split 會 random 選擇, 導致 train 和 validation 會相近而看不出差異 Input Data PreprocessingUdacity 建議我們可以處理幾個方向 將 data 數量弄得較 balance NN 算 loss 的時候不會根據每個類別數量的多寡作權重, 因此最單純的方法是就想辦法產生出一樣多的數量, 如第2點 可以增加 fake data 我的 image processing 實在很弱, 只單純的使用 rotation, 而且只敢稍微讓angle為正負5度, 怕那種有方向箭頭的號誌轉壞 1cv2.getRotationMatrix2D(image_center, angle, scale) 這樣的方式我實驗起來其實沒啥幫助, XD 我看到有人還使用 cv2.WarpPerspective, 果然專業多了! 我相信產生種類夠多的 fake data 一定會有幫助, 例如加 noise, blur 等等 將 data 做 normalization 做語音習慣了, 直覺就用 guassian normalization, mean=0, var=1, 結果整個大失敗! 只有不到1%辨識率, why?? 後來用 mean substraction, 然後除 abs 的最大值, 我只選擇使用 YUV 的 Y channel 當 input CNN 架構要設計和調整架構有點花時間, 加上我時間不多(懶), 所以我直接就用LeNet架構1234567layer_depth = &#123; 'layer_1': 6, 'layer_2': 16, 'fully_connected_1': 120, 'fully_connected_2': 84, 'out': n_classes,&#125; 自己多加了 dropout 和 l2 regularization, 原因是每次跑 training 的 accuracy 都要標到98 99, 但是 validation set 始終很難突破 93, 一直有 overfit 的感覺tensorflow 的 dropout 是設定要保留多少比例 (keep_prob), 在 training 的時候設定在最後的兩層 fully connected layers, keep_prob 愈小基本上愈難訓練也需要愈多 epoch另外記得在做 evaluation 的時候要把 keep_prob 設定成回 1 [1] 的架構想法不錯, 將較低層的 conv. layer 和較上層的 conv. layer 一併當作 fully connected layer 的 input, 這樣同時能夠有 low-level feature, higher-resolution 和 high-level feature, lower-resolution 兩種資訊一起當決策 其他 Hyper-parameters Optimizer: 說實話, 要不停的調整出最好的參數實在沒那個心力, 所以與其用SGD, 我就直接用 Adam 了 (Adagrad也是一種懶人選擇) pooling: 沒啥特別選, 因此用 max-pooling batch-size: 原先設定128, 有一次改成256就實在train不好, 就退回128了 learning rate: 0.001 l2 weight: 0.01 Learning Performancetest set accuracy = 0.893 自選測試圖片Udacity希望能學員自己找圖片來測試, 因此我就在德國的 google map 上找圖, (看著看著心都飄過去了)20張圖辨識結果如下:剛好錯10個, 只有 50% 正確率, 這實在有點悲劇其中有兩個錯誤值得注意右圖是top5辨識到的類別及機率, 可以發現除了正確答案的 traffic signal 在第二名外, 第一名的 general causion 其實跟 traffic signal 超像的 (只看灰階)看來必須把 input 的色彩資訊也加進去才能進一步改善了另一個是如下這個錯誤自己分析的原因是因為 training data 的 speed limit 都是圓的外框, 而此case剛好是一個長方形牌子, 裡面才是退色很嚴重的圓形, 所以導致辨識失敗或許真的 train 得很好的 CNN 有能力找出重要的判斷資訊, 因此會去忽略外面的方框, 而選擇去”看”外面退色的圓形和裡面的數字結論就是, 應該是我自己沒train好吧 ?! 短結小小做過一輪交通號誌辨識, 才比較有感覺真實狀況會有多困難阿~找時間來 visualize 一下每層的 hidden units 對什麼樣的 image 會有較高的 activation! This paper by Zeiler and Fergus with toolbox 要能 train 出好 model 除了參考文獻培養對 model 架構的好直覺外, engineering 的苦工也會是很大的關鍵! 後續嘗試對於目前的辨識率很不滿意. 不死心下就實作[1]的架構, 然後將 NN 的 model size 擴大, 並且將顏色資訊 YUV 的 U 加進去訓練 (結果上述因顏色錯誤的traffic signal就分對了)12345678910111213# Hyper-parametersEPOCHS = 30BATCH_SIZE = 128rate = 0.001drop_out_keep_prob = 0.5layer_depth = &#123; 'layer_1': 16, 'layer_2': 32, 'fully_connected_1': 256, 'fully_connected_2': 128, 'out': n_classes,&#125; 得到了 Test Accuracy = 0.953 ! 但是自選圖雖有進步仍很低 65%另外, 上述的參數設定下, 如果加了 l2_weight = 0.01 的話, validation 只能到 0.91x, 實在不大好訓練, 後來只好放棄第一次的 submission, reviewer 給了一些不錯的 reference 如下: Extra Important MaterialLately on slack few students asked for a good Deep Learning book.So after lot of research found a book which is also recommended by Elon Musk Deep Learning (Adaptive Computation and Machine Learning series) Github and on Amazon Fast.ai A Guide to Deep LearningFew Articles Traffic sign classification using brightness augmentation Dealing with unbalanced dataExtra Materials I noted a linkage here to discuss about how should we choose the batch_size of Stochastic Gradient Decent Since you might be interested into “Adam Optimizer”, here is a website that talks about it. You might like to learn the whole idea of Dropout It’s gives a brief analysis of the technique. reviewer 很用心阿!棒棒! Reference[1.] Traffic Sign Recognition with Multi-Scale Convolutional Networks","tags":[{"name":"ML","slug":"ML","permalink":"http://yoursite.com/tags/ML/"},{"name":"CNN","slug":"CNN","permalink":"http://yoursite.com/tags/CNN/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://yoursite.com/tags/Deep-Learning/"},{"name":"Udacity","slug":"Udacity","permalink":"http://yoursite.com/tags/Udacity/"}]},{"title":"使用AWS訓練DNN步驟","date":"2017-01-16T13:47:43.000Z","path":"2017/01/16/aws-procedure/","text":"AWS Instance注意事項及連線建立AWS instance的時候, 由於我們使用jupyter需要port 8888, 需要 Configure the Security Group Running and accessing a Jupyter notebook from AWS requires special configurations. Most of these configurations are already set up on the udacity-carnd AMI. However, you must also configure the security group correctly when you launch the instance. By default, AWS restricts access to most ports on an EC2 instance. In order to access the Jupyter notebook, you must configure the AWS Security Group to allow access to port 8888.Click on “Edit security groups”.On the “Configure Security Group” page:Select “Create a new security group”Set the “Security group name” (i.e. “Jupyter”)Click “Add Rule”Set a “Custom TCP Rule”Set the “Port Range” to “8888”Select “Anywhere” as the “Source”Click “Review and Launch” (again) 成功建立AWS instance之後, 開啟git bashssh -i ‘C:\\Users\\bobon\\.ssh\\MyKeyPair.pem’ carnd@54.65.11.64其中54.65.11.64是instance的ip AWS上開啟jupyter notebook kernel首先先把project clone下來, 並設定好conda env1234git clone https://github.com/udacity/CarND-Traffic-Sign-Classifier-Projectcd CarND-Traffic-Sign-Classifier-Projectconda env create -f environment.ymlsource activate CarND-Traffic-Sign-Classifier-Project 接著安裝tensorflow-gpu1pip install tensorflow-gpu opencv 安裝1conda install -c https://conda.binstar.org/menpo opencv 剛剛已經建立conda的環境, 且activate CarND-Traffic-Sign-Classifier-Project, 所以可以直接開啟kernel1jupyter notebook 在local瀏覽器上輸入http://[all ip addresses on your system]:8888/例如aws ip為54.65.11.641http://54.65.11.64:8888/ 抓取AWS上的資料下來local端在自己local的terminal上12scp -i &apos;C:\\Users\\bobon\\.ssh\\MyKeyPair.pem&apos; carnd@54.65.11.64:/home/carnd/Traffic-sign/cnn-traffic-sign* ./models/scp -i &apos;C:\\Users\\bobon\\.ssh\\MyKeyPair.pem&apos; carnd@54.65.11.64:/home/carnd/Traffic-sign/checkpoint ./models/ 接著輸入密碼即可 (carnd)","tags":[{"name":"aws","slug":"aws","permalink":"http://yoursite.com/tags/aws/"}]},{"title":"Hexo 中文顯示 and Markdown 測試","date":"2017-01-08T13:47:43.000Z","path":"2017/01/08/chinese-encoding/","text":"除了將Hexo的_config.yml 設定成 language: zh-tw 之外文章如果用UltraEdit編輯的話的話, 要使用轉換編碼, 將ASCII轉UTF-8(Unicode編輯), 中文才能正常顯示 引言測試同一個區塊的引言 內容文字, 強調 引言測試二同一個引言測試二的區塊 無序清單, item1 仍然是item1的內容 item2 item3 有序item1 item2 仍然是item2的內容 item3 1234567891011121314151617181920212223bool text(const string inPath, const string outPath)&#123; ifstream ifs(inPath.c_str()); if (!ifs) return false; ofstream ofs(outPath.c_str()); if (!ofs) return false; string line; while (getline(ifs,line)) &#123; istringstream iss(line); string token; while (iss&gt;&gt;token) &#123; cout &lt;&lt; \"&lt;Token&gt;: \" &lt;&lt; token &lt;&lt; endl; ofs &lt;&lt; \"&lt;Token&gt;: \" &lt;&lt; token &lt;&lt; endl; &#125; &#125; ofs.close(); ifs.close(); return true;&#125; 新的item? Here is an example of AppleScript: tell application &quot;Foo&quot; beep end tell Normal paragrah 以下為分隔線 上線使用三個*中間有空格 上線使用三個*中間無空格 上線使用5個*中間有空格 上線使用三個-中間有空格 數學公式測試 $$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$\\(x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\ 方法:在文章要有 &lt;script type=”text/javascript” src=”http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default“ &gt;&lt;/script&gt; 這行指令然後安装插件 Hexo-math, 安装方法如下, 依次为1$ npm install hexo-math --save 在 Hexo 文件夹中执行：1$ hexo math install 在 _config.yml 文件中添加：1plugins: hexo-math","tags":[{"name":"markdown","slug":"markdown","permalink":"http://yoursite.com/tags/markdown/"},{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"Hello World","date":"2017-01-08T13:43:07.943Z","path":"2017/01/08/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]