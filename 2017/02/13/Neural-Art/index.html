<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Neural Art | CS Blog</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">CS Blog</a><span class="subtitle">AI ML 自耕農</span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Neural Art</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2017-02-13</div><div class="post-categories"><a class="post-category-link" href="/categories/ML/">ML</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/Art/">Art</a>/<a class="post-tag-link" href="/tags/Deep-Learning/">Deep Learning</a></div></div></div><article><div class="container post"><h3 id="Art-with-Neural-Network"><a href="#Art-with-Neural-Network" class="headerlink" title="Art with Neural Network"></a>Art with Neural Network</h3><p><img src="/2017/02/13/Neural-Art/prisma.jpg" width="50%" height="50%"><br>風格, 創作這種能力在現在Alpha Go已經稱霸的時代, 目前覺得還是人類獨有的<br>不過有趣的是, 對於那些已經在 ImageNet 訓練得非常好的模型, 如: VGG-19, 我們通常已經同意模型可以辨別一些較<strong>抽象</strong>的概念<br>那麼是否模型裡, 也有具備類似風格和創作的元素呢? 又或者風格在模型裡該怎麼表達?<br>本篇文章主要是介紹這篇 <a href="http://www.robots.ox.ac.uk/~vgg/rg/papers/1508.06576v2.pdf" target="_blank" rel="external">A Neural Algorithm of Artistic Style</a> 的概念和實作, 另外一個很好的<a href="http://www.slideshare.net/ckmarkohchang/a-neural-algorithm-of-artistic-style" target="_blank" rel="external">投影片</a> by Mark Chang 也很值得參考</p>
<p>先給出範例結果, 結果 = 原始的<strong>內容</strong> + 希望的<strong>風格</strong></p>
<ul>
<li><strong>Content Image</strong><br><img src="/2017/02/13/Neural-Art/family.bmp" width="50%" height="50%"></li>
<li><strong>Style Image</strong><br><img src="/2017/02/13/Neural-Art/guernica.jpg" width="50%" height="50%"></li>
<li><strong>Result Image</strong><br><img src="/2017/02/13/Neural-Art/final.png" width="50%" height="50%"></li>
</ul>
<hr>
<h3 id="說在前頭的最佳化"><a href="#說在前頭的最佳化" class="headerlink" title="說在前頭的最佳化"></a>說在前頭的最佳化</h3><p>在講下去之前, 我們先講 NN 的事情, 一般情況, 我們是給定 input image <em>x</em>, 而參數 <em>w</em> 則是要求的變數, 同時對 <em>loss</em> (objective function) 做 optimize, 實作上就是 backprob.<br>上面講到的三種東西列出來:</p>
<ol>
<li><em>x</em>: input image (given, <strong>constant</strong>)</li>
<li><em>w</em>: NN parameters (<strong>variables</strong>)</li>
<li><em>loss</em>: objective function which is correlated to some desired measure</li>
</ol>
<p>事實上, backprob 的計算 <strong><em>x</em> and <em>w</em> 角色可以互換</strong>. 也就是將 <em>w</em> 固定為 constant, 而 <em>x</em> 變成 variables, 如此一來, 我們一樣可以用 backprob 去計算出最佳的 image <em>x</em>.<br>因此, 如果我們能將 <strong><em>loss</em> 定義得與風格和內容高度相關</strong>, 那麼求得的最佳 image <em>x</em> 就會有原始的內容和希望的風格了!<br>那麼再來就很明確了, 我們要定義出什麼是 <strong>Content Loss</strong> 和 <strong>Style Loss</strong> 了</p>
<hr>
<h3 id="Content-Loss"><a href="#Content-Loss" class="headerlink" title="Content Loss"></a>Content Loss</h3><p>針對一個已經訓練好的 model, 我們常常將它拿來做 feature extraction. 例如一個 DNN 把它最後一層辨識的 softmax 層拿掉, 而它的前一層的 response (做forward的結果), 就會是對於<strong>原始 input 的一種 encoding</strong>. 理論上也會有很好的鑑別力 (因最只差最後一層的softmax).</p>
<blockquote>
<p>Udacity 的 traffic-sign detection 也有拿 VGG-19, ResNet, 和 gooLeNet 做 feature extraction, 然後只訓練重新加上的 softmax layer 來得到很高的辨識率.</p>
</blockquote>
<p>因此, 我們可以將 forward 的 response image 當作是一種 measure content 的指標!<br>知道這個理由後, 原文公式就很好理解, 引用如下:</p>
<blockquote>
<p>So let <em>p</em> and <em>x</em> be the original image and the image that is generated and <em>P<sup>l</sup></em> and <em>F<sup>l</sup></em> their respective feature representation in layer <em>l</em>. We then define the squared-error loss between the two feature representations<br><img src="/2017/02/13/Neural-Art/content_loss.png" width="50%" height="50%"></p>
</blockquote>
<p>簡單來說 <em>P<sup>l</sup></em> 是 content image <em>P</em> 在 <em>l</em> 層的 response, 而 <em>F<sup>l</sup></em> 是 input image <em>x</em> (記得嗎? 它是變數喔) 在 <em>l</em> 層的 response.<br>這兩個 responses 的 squared-error 定義為 content loss, 要愈小愈好. 由於 response 為 input 的某種 encoded feature, 所以它們如果愈接近, input 就會愈接近了 (content就愈接近).<br>引用 Mark Chang 的<a href="http://www.slideshare.net/ckmarkohchang/a-neural-algorithm-of-artistic-style" target="_blank" rel="external">投影片</a>:<br><img src="/2017/02/13/Neural-Art/content_loss_by_Mark_Chang.png" width="50%" height="50%"></p>
<hr>
<h3 id="Style-Loss"><a href="#Style-Loss" class="headerlink" title="Style Loss"></a>Style Loss</h3><p>個人覺得最神奇的地方就在這裡了! 當時自己怎麼猜測都沒猜到可以這麼 formulate.<br>我個人的理解是基於 CNN 來解釋<br>假設對於某一層 ConvNet 的 kernel 為 w*h*k (width, hieght, depth), ConvNet 的 k 通常代表了有幾種 feature maps<br>說白一點, 有 k 種 filter responses 的結果, 例如第一種是線條類的response, 第二種是弧形類的responses … 等等<br>而風格就是這些 <strong>responses 的 correlation matrix!</strong> (實際上用 <strong>Gram matrix</strong>, 但意義類似)<br>基於我們對於 CNN 的理解, 愈後面的 layers 能處理愈抽象的概念, 因此愈後面的 Gram matrix 也就愈能代表抽象的 style 概念.<br>原文公式引用如下:</p>
<blockquote>
<p><img src="/2017/02/13/Neural-Art/style_loss.png" width="50%" height="50%"></p>
</blockquote>
<p>總之就是計算在 <em>l</em> 層上, sytle image <em>a</em> 和 input image <em>x</em> 它們的 Gram matrix 的 L2-norm 值</p>
<p>一樣再一次引用 Mark Chang 的<a href="http://www.slideshare.net/ckmarkohchang/a-neural-algorithm-of-artistic-style" target="_blank" rel="external">投影片</a>:<br><img src="/2017/02/13/Neural-Art/style_loss_by_Mark_Chang.png" width="50%" height="50%"><br>也可以去看看他的投影片, 有不同角度的解釋</p>
<hr>
<h3 id="實戰"><a href="#實戰" class="headerlink" title="實戰"></a>實戰</h3><p>主要參考此 <a href="https://github.com/log0/neural-style-painting/blob/master/TensorFlow%20Implementation%20of%20A%20Neural%20Algorithm%20of%20Artistic%20Style.ipynb" target="_blank" rel="external">gitHub</a><br>一開始 load VGG-19 model 就不說了, 主要的兩個 loss, codes 如下:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">content_loss_func</span><span class="params">(sess, model)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Content loss function as defined in the paper.</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_content_loss</span><span class="params">(p, x)</span>:</span></div><div class="line">        <span class="comment"># N is the number of filters (at layer l).</span></div><div class="line">        N = p.shape[<span class="number">3</span>]</div><div class="line">        <span class="comment"># M is the height times the width of the feature map (at layer l).</span></div><div class="line">        M = p.shape[<span class="number">1</span>] * p.shape[<span class="number">2</span>]</div><div class="line">        <span class="comment"># Interestingly, the paper uses this form instead:</span></div><div class="line">        <span class="comment">#</span></div><div class="line">        <span class="comment">#   0.5 * tf.reduce_sum(tf.pow(x - p, 2)) </span></div><div class="line">        <span class="comment">#</span></div><div class="line">        <span class="comment"># But this form is very slow in "painting" and thus could be missing</span></div><div class="line">        <span class="comment"># out some constants (from what I see in other source code), so I'll</span></div><div class="line">        <span class="comment"># replicate the same normalization constant as used in style loss.</span></div><div class="line">        <span class="keyword">return</span> (<span class="number">1</span> / (<span class="number">4</span> * N * M)) * tf.reduce_sum(tf.pow(x - p, <span class="number">2</span>))</div><div class="line">    <span class="keyword">return</span> _content_loss(sess.run(model[<span class="string">'conv4_2'</span>]), model[<span class="string">'conv4_2'</span>])</div></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Layers to use. We will use these layers as advised in the paper.</span></div><div class="line"><span class="comment"># To have softer features, increase the weight of the higher layers</span></div><div class="line"><span class="comment"># (conv5_1) and decrease the weight of the lower layers (conv1_1).</span></div><div class="line"><span class="comment"># To have harder features, decrease the weight of the higher layers</span></div><div class="line"><span class="comment"># (conv5_1) and increase the weight of the lower layers (conv1_1).</span></div><div class="line">STYLE_LAYERS = [</div><div class="line">    (<span class="string">'conv1_1'</span>, <span class="number">0.5</span>),</div><div class="line">    (<span class="string">'conv2_1'</span>, <span class="number">1.0</span>),</div><div class="line">    (<span class="string">'conv3_1'</span>, <span class="number">1.5</span>),</div><div class="line">    (<span class="string">'conv4_1'</span>, <span class="number">3.0</span>),</div><div class="line">    (<span class="string">'conv5_1'</span>, <span class="number">4.0</span>),</div><div class="line">]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">style_loss_func</span><span class="params">(sess, model)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Style loss function as defined in the paper.</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_gram_matrix</span><span class="params">(F, N, M)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        The gram matrix G.</div><div class="line">        """</div><div class="line">        Ft = tf.reshape(F, (M, N))</div><div class="line">        <span class="keyword">return</span> tf.matmul(tf.transpose(Ft), Ft)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_style_loss</span><span class="params">(a, x)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        The style loss calculation.</div><div class="line">        """</div><div class="line">        <span class="comment"># N is the number of filters (at layer l).</span></div><div class="line">        N = a.shape[<span class="number">3</span>]</div><div class="line">        <span class="comment"># M is the height times the width of the feature map (at layer l).</span></div><div class="line">        M = a.shape[<span class="number">1</span>] * a.shape[<span class="number">2</span>]</div><div class="line">        <span class="comment"># A is the style representation of the original image (at layer l).</span></div><div class="line">        A = _gram_matrix(a, N, M)</div><div class="line">        <span class="comment"># G is the style representation of the generated image (at layer l).</span></div><div class="line">        G = _gram_matrix(x, N, M)</div><div class="line">        result = (<span class="number">1</span> / (<span class="number">4</span> * N**<span class="number">2</span> * M**<span class="number">2</span>)) * tf.reduce_sum(tf.pow(G - A, <span class="number">2</span>))</div><div class="line">        <span class="keyword">return</span> result</div><div class="line"></div><div class="line">    E = [_style_loss(sess.run(model[layer_name]), model[layer_name]) <span class="keyword">for</span> layer_name, _ <span class="keyword">in</span> STYLE_LAYERS]</div><div class="line">    W = [w <span class="keyword">for</span> _, w <span class="keyword">in</span> STYLE_LAYERS]</div><div class="line">    loss = sum([W[l] * E[l] <span class="keyword">for</span> l <span class="keyword">in</span> range(len(STYLE_LAYERS))])</div><div class="line">    <span class="keyword">return</span> loss</div></pre></td></tr></table></figure>
<p>一開始給定 random input image:<br><img src="/2017/02/13/Neural-Art/random_input.png" width="50%" height="50%"><br>style image 選定如下:<br><img src="/2017/02/13/Neural-Art/style_image.png" width="50%" height="50%"><br>隨著 iteration 增加會像這樣:</p>
<ul>
<li>第一次的 backprob:<br><img src="/2017/02/13/Neural-Art/0.png" width="50%" height="50%"></li>
<li>1000 iteration:<br><img src="/2017/02/13/Neural-Art/1000.png" width="50%" height="50%"></li>
<li>2000 iteration:<br><img src="/2017/02/13/Neural-Art/2000.png" width="50%" height="50%"></li>
<li>3000 iteration:<br><img src="/2017/02/13/Neural-Art/3000.png" width="50%" height="50%"></li>
<li>4000 iteration:<br><img src="/2017/02/13/Neural-Art/4000.png" width="50%" height="50%"></li>
<li>5000 iteration:<br><img src="/2017/02/13/Neural-Art/5000.png" width="50%" height="50%"></li>
</ul>
<hr>
<h3 id="短節"><a href="#短節" class="headerlink" title="短節"></a>短節</h3><p>這之間很多參數可以調整去玩, 有興趣可以自己下載 <a href="https://github.com/log0/neural-style-painting/blob/master/TensorFlow%20Implementation%20of%20A%20Neural%20Algorithm%20of%20Artistic%20Style.ipynb" target="_blank" rel="external">gitHub</a> 去測</p>
<blockquote>
<p>上一篇的 “GTX 1070 參見” 有提到, 原來用 CPU 去計算, 1000 iteration 花了<strong>六個小時</strong>! 但是強大的 GTX 1070 只需要 <strong>6 分鐘</strong>!</p>
</blockquote>
<p>不過, 就算是給手機用上GTX1070好了 (哈哈當然不可能), 6分鐘的一個結果也是無法接受!<br><a href="https://prisma-ai.com/" target="_blank" rel="external">PRISMA</a> 可以在一分鐘內處理完! 這必定不是這種要算 optimization 的方法可以達到的.<br>事實上, 李飛飛的團隊發表了一篇論文 “<a href="https://cs.stanford.edu/people/jcjohns/papers/fast-style/fast-style-supp.pdf" target="_blank" rel="external">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a>“<br>訓練過後, 只需要做 forward propagation 即可! Standford University 的 JC Johnson 的 <a href="https://github.com/jcjohnson/fast-neural-style" target="_blank" rel="external">gitHub</a> 有完整的 source code!<br>找時間再來寫這篇心得文囉!</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="/atom.xml" target="_blank"><i class="fa fa-rss"></i></a></div><div class="footer">© 2017 <a href="/" rel="nofollow">Bobon</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>