<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Driving by Learning Your Style | CS Blog</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">CS Blog</a><span class="subtitle">AI ML 自耕農</span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Driving by Learning Your Style</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2017-02-05</div><div class="post-categories"><a class="post-category-link" href="/categories/ML/">ML</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/CNN/">CNN</a>/<a class="post-tag-link" href="/tags/Deep-Learning/">Deep Learning</a>/<a class="post-tag-link" href="/tags/ML/">ML</a>/<a class="post-tag-link" href="/tags/Udacity/">Udacity</a></div></div></div><article><div class="container post"><h3 id="Udacity-Self-Driving-Project-3-behavioral-cloning"><a href="#Udacity-Self-Driving-Project-3-behavioral-cloning" class="headerlink" title="Udacity Self Driving Project 3: behavioral cloning"></a>Udacity Self Driving Project 3: behavioral cloning</h3><p>A great simulator is provided that can log your driving data (speed, throttle, brake, steering, and images) and test the driving algorithm.<br>For those driving log data, steering and images are the most important features that we are going to used in this project.</p>
<blockquote>
<p><strong>The goal is, given an image, find out the corresponding steering angle.</strong></p>
</blockquote>
<p>Some might wonder that speed, throttle, and brake are features that should help.<br>Also, driving images are time correlated, not just a given static image.<br>With ignoring so much useful information, does the goal still reasonable?<br><a href="https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf" target="_blank" rel="external">Nvidia</a> just showed it works! and works pretty well!<br>So our first step is to collect training data. Fortunately, Udacity provided data for us and I used those data for training.</p>
<h3 id="Training-Data-Analysis"><a href="#Training-Data-Analysis" class="headerlink" title="Training Data Analysis"></a>Training Data Analysis</h3><p>8036 data are provided. Each data has <strong>3</strong> positions of images (left, center, right) with <strong>1</strong> corresponding steering angle.<br>Most of angles are 0, and I found that randomly ignoring half of 0-angle data is fine and can speed up. Moreover, I duplicated data that has angles among the range +-[0.2, 1] in order to balance the data.<br>Result histogram before/after data selection is showed below:<br><!--![histogram of steering angle before/after selection](steer_hist_before_after_selected_0.2_1.png)--><br><img src="/2017/02/05/Driving-by-Learning-Your-Style/steer_hist_before_after_selected_0.2_1.png" alt="histogram of steering angle before/after selection" height="80%" width="80%"></p>
<h3 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h3><p>Data augmentation is a practical way to avoid overfit and generalized the model. I used 6 types of augmentations:</p>
<ol>
<li><strong>Flipping</strong> – Flipping is a useful way to <em>balancing both turns</em> of data. For each training data, a 1/2 probability is used to decide wheter to flip. Steering angle is multiplied by -1.</li>
<li><strong>Horizontal shift</strong> – [-20,+20] pixels are randomly selected as the shift value. By doing so, it can help to recover the vehicle when it goes outside the lane.<br>By referencing this <a href="https://chatbotslife.com/using-augmentation-to-mimic-human-driving-496b569760a9#.zem65mq24" target="_blank" rel="external">article</a>, I added 0.004 steering angle units per pixel shift to the right, and subtracted 0.004 steering angle units per pixel shift to the left.<br>[-0.8~+0.8] steering values are adjusted which corresponding to [-2~+2] degrees (because steering value * 25 is the degree by setting)<!--![horizontal shift](hshift.png)-->
<img src="/2017/02/05/Driving-by-Learning-Your-Style/hshift.png" alt="horizontal shift" height="50%" width="50%"></li>
<li><strong>Brightness</strong> – The brightness is done in the “HSV” domain. I found that with a ratio in [0.5~1.1] for “V” value works fine.<!--![brightness](brightness.png)-->
<img src="/2017/02/05/Driving-by-Learning-Your-Style/brightness.png" alt="brightness" height="50%" width="50%"></li>
<li><strong>Blurring</strong> – A Gaussian blurring with kernel size 3 is applied. Not sure how useful of this method helps for robustness.<!--![blur](blur.png)-->
<img src="/2017/02/05/Driving-by-Learning-Your-Style/blur.png" alt="blur" height="50%" width="50%"></li>
<li><strong>Shadowing</strong> – A random shadowing function is adopted which is referenced from <a href="https://github.com/windowsub0406/SelfDrivingCarND/blob/master/SDC_project_3/model.ipynb" target="_blank" rel="external">this work</a>.<!--![v](shadow.png)-->
<img src="/2017/02/05/Driving-by-Learning-Your-Style/shadow.png" alt="shadow" height="50%" width="50%"></li>
<li><strong>Left/Right camera images</strong> – These left/right images are very useful for data augmentation and also help for recovering off-lane driving.<blockquote>
<p>Udacity: You also might wonder why there are three cameras on the car: center, left, and right. That’s because of the issue of recovering from being off-center.<br>In the simulator, you can weave all over the road and turn recording on and off. In a real car, however, that’s not really possible. At least not legally.<br>So in a real car, we’ll have multiple cameras on the vehicle, and we’ll map recovery paths from each camera. </p>
</blockquote>
</li>
</ol>
<p>I adjusted the steering angles for left/right images with a naive method. Following figure shows how I correct the angle of right image:<br><!--![left_right_angle_correction](left_right_angle_correction.png)--><br><img src="/2017/02/05/Driving-by-Learning-Your-Style/left_right_angle_correction.png" alt="left_right_angle_correction" height="70%" width="70%"><br>I found that setting offset = 6 or 5 is good enough. For large value, the car starts zig-zagging. An example of correction shows below:<br><img src="/2017/02/05/Driving-by-Learning-Your-Style/steer_correction.png" alt="steer_correction"></p>
<h3 id="Data-Normalization"><a href="#Data-Normalization" class="headerlink" title="Data Normalization"></a>Data Normalization</h3><ul>
<li>Normalization – Images are normalized with (x-128)/128.</li>
<li>Cropping – Images are trimmed with 40, 20, 20, and 20 pixels from top, bottom, left, and right respectively. This will cut most of the car hood and sky.</li>
<li>Resizing – resized to 66 x 200, same as NVIDIA CNN.</li>
</ul>
<h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p>I adopted NVIDIA CNN with dropout layers:<br><!--![cnn_architecture](cnn_architecture.png)--><br><img src="/2017/02/05/Driving-by-Learning-Your-Style/cnn_architecture.png" alt="cnn_architecture" height="70%" width="70%"><br><img src="/2017/02/05/Driving-by-Learning-Your-Style/cnn_architecture2.png" alt="cnn_architecture2" height="70%" width="70%"><br><!--
| Layer (type) | Output Shape | Param |
| ------------ |--------------| ------|
| Conv2D-1 | (None,31,98,24) | 1824 |
| ELU | (None,31,98,24) | 0 |
| Conv2D-2 | (None,14,47,36) | 21636 |
| ELU | (None,14,47,36) | 0 |
| Conv2D-3 | (None,5,22,48) | 43248 |
| Dropout | (None,5,22,48) | 0 |
| ELU | (None,5,22,48) | 0 |
| Conv2D-4 | (None,3,20,64) | 27712 |
| Dropout | (None,3,20,64) | 0 |
| ELU | (None,5,22,48) | 0 |
| Conv2D-5 | (None,1,18,64) | 36928 |
| Dropout | (None,1,18,64) | 0 |
| ELU | (None,1,18,64) | 0 |
| Flatten | (None,1152) | 0 |
| Dense | (None,100) | 115300 |
| ELU | (None,100) | 0 |
| Dense | (None,50) | 5050 |
| ELU | (None,50) | 0 |
| Dense | (None,10) | 510 |
| ELU | (None,10) | 0 |
| Dense | (None,1) | 11 |
Total parameters 252,219
--></p>
<h3 id="Generator-and-Training"><a href="#Generator-and-Training" class="headerlink" title="Generator and Training"></a>Generator and Training</h3><ul>
<li><strong>Generator</strong>: It is very useful to use a python generator to feed the training data batch-by-batch rather than loading all the data in memory at once.<br>A useful link to learn python iterator/generator list <a href="http://anandology.com/python-practice-book/iterators.html" target="_blank" rel="external">here</a>. For those who doesn’t familiar with python just like me :).</li>
</ul>
<blockquote>
<p>In order to further speed up. I tried pre-load a chunck of data, e.g. 5000 images, into memory, and load another chunck if the batch data (required by generator) is outside the chunck in memory. However, it <strong>does not</strong> speed up! Somewhat weired.</p>
</blockquote>
<p>For each input images, a position is randomly chosen (left,center,right).<br>Then flipping and shadowing are applied with a random fair coin. Finally, brighteness and horizonal shift are applied.<br>Steering angle adjustment is done accordingly!</p>
<ul>
<li><strong>Training</strong>: Some hyper-parameters are listed:<ul>
<li>epoch–50</li>
<li>samples for each epoch – 8896</li>
<li>optimizer – Adam with 1e-4</li>
<li>batch-size – 64</li>
</ul>
</li>
</ul>
<p>Although Keras did shuffle, it only applies in the batched data. So I shuffled the entire training set for each epoch to get more <em>de-correlated</em> data.</p>
<h3 id="Driving-Policy"><a href="#Driving-Policy" class="headerlink" title="Driving Policy"></a>Driving Policy</h3><p>I found that instead of giving a constant throttle, controlling to a constant speed is more stable to drive.<br>So I used a simple policy that tries to keep speed to 20.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">speed = float(speed)</div><div class="line">    <span class="keyword">if</span> speed &gt; <span class="number">25</span>:</div><div class="line">        throttle = <span class="number">0.05</span></div><div class="line">    <span class="keyword">elif</span> speed &gt; <span class="number">20</span>:</div><div class="line">        throttle = <span class="number">0.2</span></div><div class="line">    <span class="keyword">elif</span> speed &gt; <span class="number">10</span>:</div><div class="line">        throttle = <span class="number">0.35</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        throttle = <span class="number">0.5</span></div></pre></td></tr></table></figure></p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3></div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="/atom.xml" target="_blank"><i class="fa fa-rss"></i></a></div><div class="footer">© 2017 <a href="/" rel="nofollow">Bobon</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>