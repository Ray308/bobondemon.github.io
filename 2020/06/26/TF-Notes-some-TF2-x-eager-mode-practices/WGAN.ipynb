{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN\n",
    "\n",
    "can reference [here](https://github.com/KUASWoodyLIN/TF2-WGAN/blob/master/train.py)\n",
    "\n",
    "and [here](https://github.com/timsainb/tensorflow2-generative-models/blob/master/3.0-WGAN-GP-fashion-mnist.ipynb)\n",
    "\n",
    "[here](https://github.com/HCMY/TF2-GAN/blob/master/implementations/wgan/wgan.py)\n",
    "\n",
    "Keywords :\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import PIL\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Data Loading and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnimeDataset: (96, 96, 3)\n",
    "# extra_data: (64, 64, 3)\n",
    "imgRootPath = Path('./dataset')\n",
    "BSIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(filename):\n",
    "    image = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, [64, 64])\n",
    "    image = (image-0.5)*2.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files(str(imgRootPath/'**/*.jpg')).shuffle(buffer_size=70171)\\\n",
    "                    .map(parse_image,num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                    .batch(BSIZE).cache().prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images in train_dataset:\n",
    "    plt.imshow(images[0].numpy()/2.0 + 0.5)\n",
    "    print(images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images in train_dataset:\n",
    "    print(np.max(images[0].numpy()))\n",
    "    print(np.min(images[0].numpy()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(Model):\n",
    "#     def __init__(self):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.d1 = layers.Dense(16*16*128)\n",
    "#         self.bn1 = layers.BatchNormalization()\n",
    "#         self.relu1 = layers.ReLU()\n",
    "        \n",
    "#         self.reshape2 = layers.Reshape((16,16,128))\n",
    "        \n",
    "#         self.deconv3 = layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', use_bias=False)\n",
    "#         self.bn3 = layers.BatchNormalization()\n",
    "#         self.relu3 = layers.ReLU()\n",
    "        \n",
    "#         self.deconv4 = layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False)\n",
    "#         self.bn4 = layers.BatchNormalization()\n",
    "#         self.relu4 = layers.ReLU()\n",
    "        \n",
    "#         self.deconv5 = layers.Conv2DTranspose(3, (4, 4), strides=(1, 1), padding='same', use_bias=False)\n",
    "        \n",
    "#     def call(self,x):\n",
    "#         x = self.d1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "        \n",
    "#         x = self.reshape2(x)\n",
    "        \n",
    "#         x = self.deconv3(x)\n",
    "#         x = self.bn3(x)\n",
    "#         x = self.relu3(x)\n",
    "        \n",
    "#         x = self.deconv4(x)\n",
    "#         x = self.bn4(x)\n",
    "#         x = self.relu4(x)\n",
    "        \n",
    "#         x = self.deconv5(x)\n",
    "        \n",
    "#         return tf.keras.activations.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(input_shape=(1, 1, 100), name='Generator'):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # 1: 1x1 -> 4x4\n",
    "    x = keras.layers.Conv2DTranspose(512, 4, strides=1, padding='valid', use_bias=False)(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    # 2: 4x4 -> 8x8\n",
    "    x = keras.layers.Conv2DTranspose(256, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    # 3: 8x8 -> 16x16\n",
    "    x = keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    # 4: 16x16 -> 32x32\n",
    "    x = keras.layers.Conv2DTranspose(64, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    # 5: 32x32 -> 64x64\n",
    "    x = keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    outputs = keras.layers.Activation('tanh')(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.build(input_shape=(None,100))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator(Model):\n",
    "#     def __init__(self):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.conv1 = layers.Conv2D(32, 4, strides=(2, 2), padding='same')\n",
    "#         self.bn1 = layers.BatchNormalization()\n",
    "#         self.relu1 = layers.ReLU()\n",
    "        \n",
    "#         self.conv2 = layers.Conv2D(64, 4, strides=(2, 2), padding='same')\n",
    "#         self.bn2 = layers.BatchNormalization()\n",
    "#         self.relu2 = layers.ReLU()\n",
    "        \n",
    "#         self.conv3 = layers.Conv2D(128, 4, strides=(2, 2), padding='same')\n",
    "#         self.bn3 = layers.BatchNormalization()\n",
    "#         self.relu3 = layers.ReLU()\n",
    "        \n",
    "#         self.conv4 = layers.Conv2D(256, 4, strides=(2, 2), padding='same')\n",
    "#         self.bn4 = layers.BatchNormalization()\n",
    "#         self.relu4 = layers.ReLU()\n",
    "        \n",
    "#         self.flatten5 = layers.Flatten()\n",
    "        \n",
    "#         self.d6 = layers.Dense(1)\n",
    "        \n",
    "#     def call(self,x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "        \n",
    "#         x = self.conv3(x)\n",
    "#         x = self.bn3(x)\n",
    "#         x = self.relu3(x)\n",
    "        \n",
    "#         x = self.conv4(x)\n",
    "#         x = self.bn4(x)\n",
    "#         x = self.relu4(x)\n",
    "        \n",
    "#         x = self.flatten5(x)\n",
    "#         x = self.d6(x)\n",
    "        \n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(input_shape=(64, 64, 3), name='Discriminator'):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # 1: 64x64 -> 32x32\n",
    "    x = keras.layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    # 2: 32x32 -> 16x16\n",
    "    x = keras.layers.Conv2D(128, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    # 3: 16x16 -> 8x8\n",
    "    x = keras.layers.Conv2D(256, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    # 4: 8x8 -> 4x4\n",
    "    x = keras.layers.Conv2D(512, 4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    # 5: 4x4 -> 1x1\n",
    "    outputs = keras.layers.Conv2D(1, 4, strides=1, padding='valid')(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.build(input_shape=(None,64,64,3))\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = -tf.reduce_mean(real_output)\n",
    "    fake_loss = tf.reduce_mean(fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "# generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)\n",
    "# discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5, beta_2=0.9)\n",
    "checkpoint_dir = Path('./training_checkpoints')\n",
    "checkpoint_prefix = str(checkpoint_dir/\"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIM = 100\n",
    "D_ITR = 5\n",
    "EPOCHS = 700\n",
    "clamp_lower = -0.01\n",
    "clamp_upper = 0.01\n",
    "num_examples_to_generate = 16\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, 1, 1, NOISE_DIM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_dis_step(images):\n",
    "    noise = tf.random.normal([BSIZE, 1, 1, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    # apply weight clipping\n",
    "    # discriminator.trainable_variables including bn?? Y\n",
    "    for var in discriminator.trainable_variables:\n",
    "#         tf.compat.v1.assign(var,tf.clip_by_value(var, clamp_lower, clamp_upper))\n",
    "        var.assign(tf.clip_by_value(var, clamp_lower, clamp_upper))\n",
    "#     ws = [tf.clip_by_value(w, clamp_lower, clamp_upper) for w in discriminator.get_weights()]\n",
    "#     discriminator.set_weights(ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator.trainable = True\n",
    "# varlist = discriminator.trainable_variables\n",
    "# names = [var.name for var in varlist]\n",
    "# print(names)\n",
    "\n",
    "# print(varlist[-1])\n",
    "# varlist = discriminator.trainable_variables\n",
    "# varlist[-1] = tf.clip_by_value(varlist[-1], -0.01, 0.01)\n",
    "# print(varlist[-1])\n",
    "# print(discriminator.trainable_variables[-1])\n",
    "\n",
    "# ws = discriminator.get_weights()\n",
    "# print(ws[-1])\n",
    "# ws[-1] = tf.clip_by_value(ws[-1], -0.01, 0.01)\n",
    "# print(ws[-1])\n",
    "# discriminator.set_weights(ws)\n",
    "# print(discriminator.get_weights()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_gen_step():\n",
    "    noise = tf.random.normal([BSIZE, 1, 1, NOISE_DIM])\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        counter = 1\n",
    "        for image_batch in dataset:\n",
    "            train_dis_step(image_batch)\n",
    "            ws = discriminator.get_weights()\n",
    "#             print(ws[-1])\n",
    "            if counter%D_ITR==0:\n",
    "                train_gen_step()\n",
    "            counter += 1\n",
    "\n",
    "        # Produce images for the GIF as we go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,\n",
    "                                 epoch + 1,\n",
    "                                 seed)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i].numpy()/2.0 + 0.5)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('out_images/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "    return PIL.Image.open('out_images/image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(446)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `imageio` to create an animated gif using the images saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'all_rms_wgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = glob.glob('out_images/image*.png')\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1\n",
    "    for i,filename in enumerate(filenames):\n",
    "        frame = 10*(i**0.5)\n",
    "        if round(frame) > round(last):\n",
    "            last = frame\n",
    "        else:\n",
    "            continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "    display.Image(filename=anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download(anim_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try generating other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = np.random.randn(num_examples_to_generate, 1, 1, NOISE_DIM)\n",
    "predictions = generator(codes, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(predictions[i].numpy()/2.0 + 0.5)\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
