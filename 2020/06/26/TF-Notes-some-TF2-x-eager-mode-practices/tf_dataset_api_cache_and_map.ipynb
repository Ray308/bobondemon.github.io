{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data.Dataset `map()`, `cache()`, and `shuffle()` notes\n",
    "\n",
    "TF recommends following tips [https://www.tensorflow.org/guide/data_performance#overview]\n",
    "\n",
    ">Here is a summary of the best practices for designing performant TensorFlow input pipelines:\n",
    ">\n",
    ">- Use the `prefetch` transformation to overlap the work of a producer and consumer.\n",
    "- Parallelize the data reading transformation using the `interleave` transformation.\n",
    "- Parallelize the `map` transformation by setting the `num_parallel_calls` argument.\n",
    "- Use the `cache` transformation to cache data in memory during the first epoch\n",
    "- Vectorize user-defined functions passed in to the `map` transformation\n",
    "- Reduce memory usage when applying the `interleave`, `prefetch`, and `shuffle` transformations.\n",
    "\n",
    "Note that TF suggests `dataset.map(func).cache()` when `func` is a time-consuming op.\n",
    "\n",
    "However, if there has random ops in `func`, data will be affected by random op **only once** (first epoch). No randomized data in the rest of epoch occurs.\n",
    "\n",
    "In conclusion:\n",
    "  - if `map()` has random ops: `dataset.shuffle().batch().cache().map().prefetch()`\n",
    "  - `map()` has NO random ops: `dataset.shuffle().batch().map().cache().prefetch()`\n",
    "\n",
    "Let's verify with tensorflow ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawvalue = np.arange(8,dtype=np.float32)\n",
    "rawindex = np.arange(8,dtype=np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `map()` Augmentation with `tf.random`\n",
    "Every epoch of dataset is **RANDOM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tf_randn(v,i):\n",
    "    return (tf.add(v,tf.random.truncated_normal(shape=())),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((rawvalue,rawindex)).batch(2)\n",
    "# suggest using `num_parallel_calls=tf.data.experimental.AUTOTUNE in practice`\n",
    "dataset = dataset.map(map_tf_randn, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 + randn = 0.7018770575523376;\n",
      "1 + randn = 1.7018771171569824;\n",
      "2 + randn = 3.3502042293548584;\n",
      "3 + randn = 4.3502044677734375;\n",
      "4 + randn = 2.3089733123779297;\n",
      "5 + randn = 3.3089733123779297;\n",
      "6 + randn = 6.10546875;\n",
      "7 + randn = 7.10546875;\n"
     ]
    }
   ],
   "source": [
    "for data, index in dataset:\n",
    "    for d,i in zip(data,index):\n",
    "        print(f'{i} + randn = {d};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 + randn = 0.2537071406841278;\n",
      "1 + randn = 1.2537071704864502;\n",
      "2 + randn = 1.8037147521972656;\n",
      "3 + randn = 2.8037147521972656;\n",
      "4 + randn = 3.758244514465332;\n",
      "5 + randn = 4.758244514465332;\n",
      "6 + randn = 4.913802623748779;\n",
      "7 + randn = 5.913802623748779;\n"
     ]
    }
   ],
   "source": [
    "for data, index in dataset:\n",
    "    for d,i in zip(data,index):\n",
    "        print(f'{i} + randn = {d};')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `map()` Augmentation with `np.random`\n",
    "Every epoch of dataset is **NOT RANDOM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_np_randn(v,i):\n",
    "    return (tf.add(v,np.random.randn()),i)\n",
    "\n",
    "dataset_np_randn = tf.data.Dataset.from_tensor_slices((rawvalue,rawindex)).batch(2)\n",
    "dataset_np_randn = dataset_np_randn.map(map_np_randn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 + randn = 1.3981781005859375;\n",
      "1 + randn = 2.3981781005859375;\n",
      "2 + randn = 3.3981781005859375;\n",
      "3 + randn = 4.3981781005859375;\n",
      "4 + randn = 5.3981781005859375;\n",
      "5 + randn = 6.3981781005859375;\n",
      "6 + randn = 7.3981781005859375;\n",
      "7 + randn = 8.398178100585938;\n"
     ]
    }
   ],
   "source": [
    "for data, index in dataset_np_randn:\n",
    "    for d,i in zip(data,index):\n",
    "        print(f'{i} + randn = {d};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 + randn = 1.3981781005859375;\n",
      "1 + randn = 2.3981781005859375;\n",
      "2 + randn = 3.3981781005859375;\n",
      "3 + randn = 4.3981781005859375;\n",
      "4 + randn = 5.3981781005859375;\n",
      "5 + randn = 6.3981781005859375;\n",
      "6 + randn = 7.3981781005859375;\n",
      "7 + randn = 8.398178100585938;\n"
     ]
    }
   ],
   "source": [
    "for data, index in dataset_np_randn:\n",
    "    for d,i in zip(data,index):\n",
    "        print(f'{i} + randn = {d};')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using `cache()` *after* `map()`\n",
    "Every epoch of dataset is **NOT RANDOM** (even we use `tf.random`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cache = tf.data.Dataset.from_tensor_slices((rawvalue,rawindex)).batch(2)\n",
    "dataset_cache = dataset_cache.map(map_tf_randn).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 + randn = 1.3161203861236572;\n",
      "1 + randn = 2.3161203861236572;\n",
      "2 + randn = 2.256491184234619;\n",
      "3 + randn = 3.256491184234619;\n",
      "4 + randn = 2.33833646774292;\n",
      "5 + randn = 3.33833646774292;\n",
      "6 + randn = 7.918804168701172;\n",
      "7 + randn = 8.918804168701172;\n"
     ]
    }
   ],
   "source": [
    "for data, index in dataset_cache:\n",
    "    for d,i in zip(data,index):\n",
    "        print(f'{i} + randn = {d};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 + randn = 1.3161203861236572;\n",
      "1 + randn = 2.3161203861236572;\n",
      "2 + randn = 2.256491184234619;\n",
      "3 + randn = 3.256491184234619;\n",
      "4 + randn = 2.33833646774292;\n",
      "5 + randn = 3.33833646774292;\n",
      "6 + randn = 7.918804168701172;\n",
      "7 + randn = 8.918804168701172;\n"
     ]
    }
   ],
   "source": [
    "for data, index in dataset_cache:\n",
    "    for d,i in zip(data,index):\n",
    "        print(f'{i} + randn = {d};')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using `cache()` *before* `map()`\n",
    "Every epoch of dataset is now **RANDOM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cache2 = tf.data.Dataset.from_tensor_slices((rawvalue,rawindex)).batch(2)\n",
    "dataset_cache2 = dataset_cache2.cache().map(map_tf_randn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 + randn = 1.6351758241653442;\n",
      "1 + randn = 2.6351757049560547;\n",
      "2 + randn = 1.7928622961044312;\n",
      "3 + randn = 2.7928624153137207;\n",
      "4 + randn = 4.905435562133789;\n",
      "5 + randn = 5.905435562133789;\n",
      "6 + randn = 6.691409111022949;\n",
      "7 + randn = 7.691409111022949;\n"
     ]
    }
   ],
   "source": [
    "for data, index in dataset_cache2:\n",
    "    for d,i in zip(data,index):\n",
    "        print(f'{i} + randn = {d};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 + randn = -0.5546321868896484;\n",
      "1 + randn = 0.44536781311035156;\n",
      "2 + randn = 3.8618063926696777;\n",
      "3 + randn = 4.861806392669678;\n",
      "4 + randn = 3.8405842781066895;\n",
      "5 + randn = 4.8405842781066895;\n",
      "6 + randn = 7.156463146209717;\n",
      "7 + randn = 8.156462669372559;\n"
     ]
    }
   ],
   "source": [
    "for data, index in dataset_cache2:\n",
    "    for d,i in zip(data,index):\n",
    "        print(f'{i} + randn = {d};')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. `shuffle()` machanism\n",
    "\n",
    "Sometimes data belongs to same class is gathered together while constructing dataset. Becareful the case when `buffer_size` in `shuffle()` is too small. This makes shuffle working badly. Following figure explains `shuffle` machanism and thus can see why.\n",
    "\n",
    "<img src=\"shuffle.png\" width=40% height=40% align=\"left\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
